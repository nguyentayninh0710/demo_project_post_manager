<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Call an existing REST service with Apache Camel K</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/TdDN8vPz2vY/" /><category term="application integration" scheme="searchisko:content:tags" /><category term="Camel K" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="jitpack" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="OpenAPI" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="operator" scheme="searchisko:content:tags" /><category term="serverless" scheme="searchisko:content:tags" /><author><name>Mary Cochran</name></author><id>searchisko:content:id:jbossorg_blog-call_an_existing_rest_service_with_apache_camel_k</id><updated>2020-09-28T07:00:41Z</updated><published>2020-09-28T07:00:41Z</published><content type="html">&lt;p&gt;With the &lt;a href="https://developers.redhat.com/blog/2020/06/18/camel-k-1-0-the-serverless-integration-platform-goes-ga/"&gt;release of Apache Camel K&lt;/a&gt;, it is possible to create and deploy integrations with existing applications that are quicker and more lightweight than ever. In many cases, calling an existing REST endpoint is the best way to connect a new system to an existing one. Take the example of a cafe serving coffee. What happens when the cafe wants to allow customers to use a delivery service like GrubHub? You would only need to introduce a single &lt;a href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k/"&gt;Camel K&lt;/a&gt; integration to connect the cafe and GrubHub systems.&lt;/p&gt; &lt;p&gt;In this article, I will show you how to create a &lt;a href="https://developers.redhat.com/videos/youtube/51x9BewGCYA"&gt;Camel K integration&lt;/a&gt; that calls an existing REST service and uses its existing data format. For the data format, I have a Maven project configured with &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; objects. Ideally, you would have this packaged and available in a Nexus repository. For the purpose of my demonstration, I utilized &lt;a target="_blank" rel="nofollow" href="https://jitpack.io/"&gt;JitPack&lt;/a&gt;, which lets me have my dependency available in a repository directly from my GitHub code. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/jeremyrdavis/quarkus-cafe-demo/tree/kamel-1.0.0/grubhub-cafe-core"&gt;GitHub repository associated with this demo&lt;/a&gt; for the data format code and directions for getting it into JitPack.&lt;br /&gt; &lt;span id="more-760597"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;In order to follow the demonstration, you will need the following installed in your development environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;oc&lt;/code&gt; command-line tools&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/apache/camel-k/releases"&gt;Camel K client 1.0.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4.4 cluster&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Create the Camel K route&lt;/h2&gt; &lt;p&gt;First, we create the Camel K route file, which I have named &lt;code&gt;RestWithUndertow.java&lt;/code&gt;. Here, we open the file and create the class structure:&lt;/p&gt; &lt;pre&gt;public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { } }&lt;/pre&gt; &lt;p&gt;Next, we create the REST endpoint, and we also create the data formats that we will use. In this case, we&amp;#8217;ll receive the REST request as a &lt;code&gt;GrubHubOrder&lt;/code&gt;. We&amp;#8217;ll then transform it to a &lt;code&gt;CreateOrderCommand&lt;/code&gt;, which we&amp;#8217;ll send to the REST service that is already in use:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.model.rest.RestBindingMode; import com.redhat.quarkus.cafe.domain.CreateOrderCommand; import com.redhat.grubhub.cafe.domain.GrubHubOrder; import org.apache.camel.component.jackson.JacksonDataFormat; public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { JacksonDataFormat df = new JacksonDataFormat(CreateOrderCommand.class); rest() .post("/order").type(GrubHubOrder.class).consumes("application/json") .bindingMode(RestBindingMode.json) .produces("application/json") .to("direct:order"); } } &lt;/pre&gt; &lt;h2&gt;Create the data transformation&lt;/h2&gt; &lt;p&gt;Now we can create a method in the same file, which will assist with the data transformation from the &lt;code&gt;GrubHubOrder&lt;/code&gt; to the &lt;code&gt;CreateOrderCommand&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; public void transformMessage(Exchange exchange){ Message in = exchange.getIn(); GrubHubOrder gho = in.getBody(GrubHubOrder.class); List oi = gho.getOrderItems(); List list = new ArrayList(); for(GrubHubOrderItem i : oi){ LineItem li = new LineItem(Item.valueOf(i.getOrderItem()),i.getName()); list.add(li); } CreateOrderCommand coc = new CreateOrderCommand(list, null); in.setBody(coc); } &lt;/pre&gt; &lt;p&gt;Make sure that you add the following imports to the file, as well:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.Exchange; import org.apache.camel.Message; import com.redhat.quarkus.cafe.domain.LineItem; import com.redhat.quarkus.cafe.domain.Item; import java.util.List; import java.util.ArrayList; import com.redhat.grubhub.cafe.domain.GrubHubOrderItem; &lt;/pre&gt; &lt;h2&gt;Call the existing service from your Camel K REST endpoint&lt;/h2&gt; &lt;p&gt;Now that we have a method to do the transformation, we can implement the rest of the Camel K REST endpoint and make it call the existing service. Add the following below the code that you have so far:&lt;/p&gt; &lt;pre&gt; from("direct:order") .log("Incoming Body is ${body}") .log("Incoming Body after unmarshal is ${body}") .bean(this,"transformMessage") .log("Outgoing pojo Body is ${body}") .marshal(df) //transforms the java object into json .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader(Exchange.CONTENT_TYPE, constant("application/json")) .setHeader("Accept",constant("application/json")) .log("Body after transformation is ${body} with headers: ${headers}") .to("http://?bridgeEndpoint=true&amp;#38;throwExceptionOnFailure=false") .setHeader(Exchange.HTTP_RESPONSE_CODE,constant(200)) .transform().simple("{Order Placed}"); &lt;/pre&gt; &lt;p&gt;Note that this example includes plenty of logging to give visibility to what is being done. I have set the &lt;code&gt;BridgeEndpoint&lt;/code&gt; option to &lt;code&gt;true&lt;/code&gt;, which allows us to ignore the &lt;code&gt;HTTP_URI&lt;/code&gt; incoming header and use the full URL that we&amp;#8217;ve specified. This is important when taking an incoming REST request that will call a new one. You can read more about &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/components/latest/http-component.html"&gt;the &lt;code&gt;BridgeEndpoint&lt;/code&gt; option here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Save the route file&lt;/h2&gt; &lt;p&gt;Your complete Camel K route file should look something like this:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.Exchange; import org.apache.camel.Message; import org.apache.camel.model.rest.RestBindingMode; import com.redhat.quarkus.cafe.domain.LineItem; import com.redhat.quarkus.cafe.domain.Item; import java.util.List; import java.util.ArrayList; import com.redhat.quarkus.cafe.domain.CreateOrderCommand; import com.redhat.grubhub.cafe.domain.GrubHubOrder; import com.redhat.grubhub.cafe.domain.GrubHubOrderItem; import org.apache.camel.component.jackson.JacksonDataFormat; public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { JacksonDataFormat df = new JacksonDataFormat(CreateOrderCommand.class); rest() .post("/order").type(GrubHubOrder.class).consumes("application/json") .bindingMode(RestBindingMode.json) .produces("application/json") .to("direct:order"); from("direct:order") .log("Incoming Body is ${body}") .log("Incoming Body after unmarshal is ${body}") .bean(this,"transformMessage") .log("Outgoing pojo Body is ${body}") .marshal(df) .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader(Exchange.CONTENT_TYPE, constant("application/json")) .setHeader("Accept",constant("application/json")) .log("Body after transformation is ${body} with headers: ${headers}") //need to change url after knowing what the cafe-web url will be .to("http://sampleurl.com?bridgeEndpoint=true&amp;#38;throwExceptionOnFailure=false") .setHeader(Exchange.HTTP_RESPONSE_CODE,constant(200)) .transform().simple("{Order Placed}"); } public void transformMessage(Exchange exchange){ Message in = exchange.getIn(); GrubHubOrder gho = in.getBody(GrubHubOrder.class); List oi = gho.getOrderItems(); List list = new ArrayList(); for(GrubHubOrderItem i : oi){ LineItem li = new LineItem(Item.valueOf(i.getOrderItem()),i.getName()); list.add(li); } CreateOrderCommand coc = new CreateOrderCommand(list, null); in.setBody(coc); } }&lt;/pre&gt; &lt;p&gt;Save this file and get ready for the last step.&lt;/p&gt; &lt;h2&gt;Run the integration&lt;/h2&gt; &lt;p&gt;To run your integration, you will need to have the Camel K Operator installed and ensure that it has access to the dependencies in JitPack. Do the following to get your infrastructure ready for the integration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Log in to OpenShift using the &lt;code&gt;oc&lt;/code&gt; command line.&lt;/li&gt; &lt;li&gt;Install the Camel K Operator via the OpenShift OperatorHub. The default options are fine.&lt;/li&gt; &lt;li&gt;Ensure you have &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/cli/cli.html"&gt;Kamel CLI tooling &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Use the &lt;code&gt;oc&lt;/code&gt; and &lt;code&gt;kamel&lt;/code&gt; tools and the following command to create an integration platform that provides access to JitPack: &lt;pre&gt;kamel install --olm=false --skip-cluster-setup --skip-operator-setup --maven-repository https://jitpack.io@id=jitpack@snapshots &lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once you see that everything is ready in your OpenShift console (you can always enter &lt;code&gt;oc get pods&lt;/code&gt;to check), deploy the integration. Using your Kamel tools again, make sure that you are logged into OpenShift and on the appropriate project, and run the following command:&lt;/p&gt; &lt;pre&gt;kamel run --name=rest-with-undertow --dependency=camel-jackson --dependency=mvn:com.github.jeremyrdavis:quarkus-cafe-demo:1.5-SNAPSHOT --dependency=mvn:com.github.jeremyrdavis.quarkus-cafe-demo:grubhub-cafe-core:1.5-SNAPSHOT --dependency=camel-openapi-java RestWithUndertow.java &lt;/pre&gt; &lt;p&gt;This command ensures that your route starts with all of the appropriate dependencies. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/jeremyrdavis/quarkus-cafe-demo/tree/kamel-1.0.0/camel-k-grub-hub"&gt;GitHub repository for this article&lt;/a&gt; for a complete,  working version of this code and the service that it calls.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Camel K allows you to develop your integrations quickly and efficiently while keeping your footprint small. Even when you have some dependencies for your integrations you can utilize the Knative technology in Camel K to make your integrations less resource intensive and allow for quicker deployments.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#038;title=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" data-a2a-url="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/" data-a2a-title="Call an existing REST service with Apache Camel K"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/"&gt;Call an existing REST service with Apache Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/TdDN8vPz2vY" height="1" width="1" alt=""/&gt;</content><summary>With the release of Apache Camel K, it is possible to create and deploy integrations with existing applications that are quicker and more lightweight than ever. In many cases, calling an existing REST endpoint is the best way to connect a new system to an existing one. Take the example of a cafe serving coffee. What happens when the cafe wants to allow customers to use a delivery service like Grub...</summary><dc:creator>Mary Cochran</dc:creator><dc:date>2020-09-28T07:00:41Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/</feedburner:origLink></entry><entry><title>Build a data streaming pipeline using Kafka Streams and Quarkus</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6-PYLEG5zIc/" /><category term="data pipeline" scheme="searchisko:content:tags" /><category term="data streaming" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Kafka streams" scheme="searchisko:content:tags" /><category term="kafka tutorial" scheme="searchisko:content:tags" /><category term="message processing" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="Spring Boot" scheme="searchisko:content:tags" /><category term="Stream Processing" scheme="searchisko:content:tags" /><author><name>Kapil Shukla</name></author><id>searchisko:content:id:jbossorg_blog-build_a_data_streaming_pipeline_using_kafka_streams_and_quarkus</id><updated>2020-09-28T07:00:22Z</updated><published>2020-09-28T07:00:22Z</published><content type="html">&lt;p&gt;In typical data warehousing systems, &lt;a href="https://developers.redhat.com/blog/category/big-data/"&gt;data&lt;/a&gt; is first accumulated and then processed. But with the advent of new technologies, it is now possible to process data as and when it arrives. We call this real-time data processing. In real-time processing, data streams through pipelines; i.e., moving from one system to another. Data gets generated from static sources (like databases) or real-time systems (like transactional applications), and then gets filtered, transformed, and finally stored in a database or pushed to several other systems for further processing. The other systems can then follow the same cycle—i.e., filter, transform, store, or push to other systems.&lt;/p&gt; &lt;p&gt;In this article, we will build a &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; application that streams and processes data in real-time using &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt;. As we go through the example, you will learn how to apply &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka concepts&lt;/a&gt; such as joins, windows, processors, state stores, punctuators, and interactive queries. By the end of the article, you will have the architecture for a realistic data streaming pipeline in Quarkus.&lt;/p&gt; &lt;h2&gt;The traditional messaging system&lt;/h2&gt; &lt;p&gt;As developers, we are tasked with updating a message-processing system that was originally built using a relational database and a traditional message broker. Here&amp;#8217;s the data flow for the messaging system:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Data from two different systems arrives in two different messaging queues. Each record in one queue has a corresponding record in the other queue. Each record has a unique key.&lt;/li&gt; &lt;li&gt;When a data record arrives in one of the message queues, the system uses the record&amp;#8217;s unique key to determine whether the database already has an entry for that record. If it does not find a record with that unique key, the system inserts the record into the database for processing.&lt;/li&gt; &lt;li&gt;If the same data record arrives in the second queue within a few seconds, the application triggers the same logic. It checks whether a record with the same key is present in the database. If the record is present, the application retrieves the data and processes the two data objects.&lt;/li&gt; &lt;li&gt;If the data record doesn&amp;#8217;t arrive in the second queue within 50 seconds after arriving in the first queue, then another application processes the record in the database.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As you might imagine, this scenario worked well before the advent of data streaming, but it does not work so well today.&lt;/p&gt; &lt;h2&gt;The data streaming pipeline&lt;/h2&gt; &lt;p&gt;Our task is to build a new message system that executes data streaming operations with Kafka. This type of application is capable of processing data in real-time, and it eliminates the need to maintain a database for unprocessed records. Figure 1 illustrates the data flow for the new application:&lt;/p&gt; &lt;div id="attachment_749567" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution.jpg"&gt;&lt;img aria-describedby="caption-attachment-749567" class="wp-image-749567 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-1024x394.jpg" alt="A flow diagram of the data-streaming pipeline's architecture." width="640" height="246" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-1024x394.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-300x115.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-768x295.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749567" class="wp-caption-text"&gt;Figure 1: Architecture of the data streaming pipeline.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;In the next sections, we&amp;#8217;ll go through the process of building a data streaming pipeline with Kafka Streams in Quarkus. You can get the complete source code from the article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus"&gt;GitHub repository&lt;/a&gt;. Before we start coding the architecture, let&amp;#8217;s discuss joins and windows in Kafka Streams.&lt;/p&gt; &lt;h2&gt;Joins and windows in Kafka Streams&lt;/h2&gt; &lt;p&gt;Kafka allows you to &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#joining"&gt;join&lt;/a&gt; records that arrive on two different topics. You are probably familiar with the concept of &lt;i&gt;joins&lt;/i&gt; in a relational database, where the data is static and available in two tables. In Kafka, joins work differently because the data is always streaming.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll look at the types of joins in a moment, but the first thing to note is that joins happen for data collected over a duration of time. Kafka calls this type of collection &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing"&gt;windowing&lt;/a&gt;. Various &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing"&gt;types of windows&lt;/a&gt; are available in Kafka. For our example, we will use a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing-tumbling"&gt;tumbling window&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Inner joins&lt;/h3&gt; &lt;p&gt;Now, let&amp;#8217;s consider how an inner join works. Assume that two separate data streams arrive in two different Kafka topics, which we will call the left and right topics. A record arriving in one topic has another relevant record (with the same key but a different value) that is also arriving in the other topic. The second record arrives after a brief time delay. As shown in Figure 2, we create a Kafka stream for each of the topics.&lt;/p&gt; &lt;div id="attachment_749587" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join.jpg"&gt;&lt;img aria-describedby="caption-attachment-749587" class="wp-image-749587 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-1024x337.jpg" alt="A diagram of an inner join for two topics." width="640" height="211" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-1024x337.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-300x99.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-768x253.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join.jpg 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749587" class="wp-caption-text"&gt;Figure 2: Diagram of an inner join.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The inner join on the left and right streams creates a new data stream. When it finds a matching record (with the same key) on both the left and right streams, Kafka emits a new record at time &lt;i&gt;t2&lt;/i&gt; in the new stream. Because the B record did not arrive on the right stream within the specified time window, Kafka Streams won&amp;#8217;t emit a new record for B.&lt;/p&gt; &lt;h3&gt;Outer joins&lt;/h3&gt; &lt;p&gt;Next, let&amp;#8217;s look at how an outer join works. Figure 3 shows the data flow for the outer join in our example:&lt;/p&gt; &lt;div id="attachment_749597" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join.jpg"&gt;&lt;img aria-describedby="caption-attachment-749597" class="wp-image-749597 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-1024x498.jpg" alt="A diagram of an outer join." width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-1024x498.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-300x146.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-768x374.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join.jpg 1278w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749597" class="wp-caption-text"&gt;Figure 3: Diagram of an outer join.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If we don&amp;#8217;t use the &amp;#8220;group by&amp;#8221; clause when we join two streams in Kafka Streams, then the join operation will emit three records. Streams in Kafka do not wait for the entire window; instead, they start emitting records whenever the condition for an outer join is true. So, when Record A on the left stream arrives at time &lt;i&gt;t1&lt;/i&gt;, the join operation immediately emits a new record. At time t2, the &lt;code&gt;outerjoin&lt;/code&gt; Kafka stream receives data from the right stream. The join operation immediately emits another record with the values from both the left and right records.&lt;/p&gt; &lt;p&gt;You would see different outputs if you used the &lt;code&gt;groupBy&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; functions on these Kafka streams. In that case, the streams would wait for the window to complete the duration, perform the join, and then emit the data, as previously shown in Figure 3.&lt;/p&gt; &lt;p&gt;Understanding how inner and outer joins work in Kafka Streams helps us find the best way to implement the data flow that we want. In this case, it is clear that we need to perform an outer join. This type of join allows us to retrieve records that appear in both the left and right topics, as well as records that appear in only one of them.&lt;/p&gt; &lt;p&gt;With that background out of the way, let&amp;#8217;s begin building our Kafka-based data streaming pipeline.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: We can use Quarkus extensions for Spring Web and Spring DI (dependency injection) to code in the &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; style using Spring-based annotations.&lt;/p&gt; &lt;h2&gt;Step 1: Perform the outer join&lt;/h2&gt; &lt;p&gt;To perform the outer join, we first create a class called &lt;code&gt;KafkaStreaming&lt;/code&gt;, then add the function &lt;code&gt;startStreamStreamOuterJoin()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;@RestController public class KafkaStreaming { private KafkaStreams streamsOuterJoin; private final String LEFT_STREAM_TOPIC = "left-stream-topic"; private final String RIGHT_STREAM_TOPIC = "right-stream-topic"; private final String OUTER_JOIN_STREAM_OUT_TOPIC = "stream-stream-outerjoin"; private final String PROCESSED_STREAM_OUT_TOPIC = "processed-topic"; private final String KAFKA_APP_ID = "outerjoin"; private final String KAFKA_SERVER_NAME = "localhost:9092"; @RequestMapping("/startstream/") public void startStreamStreamOuterJoin() { Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, KAFKA_APP_ID); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_SERVER_NAME); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); final StreamsBuilder builder = new StreamsBuilder(); KStream&amp;#60;String, String&amp;#62; leftSource = builder.stream(LEFT_STREAM_TOPIC); KStream&amp;#60;String, String&amp;#62; rightSource = builder.stream(RIGHT_STREAM_TOPIC); // TODO 1 - Add state store // do the outer join // change the value to be a mix of both streams value // have a moving window of 5 seconds // output the last value received for a specific key during the window // push the data to OUTER_JOIN_STREAM_OUT_TOPIC topic leftSource.outerJoin(rightSource, (leftValue, rightValue) -&amp;#62; "left=" + leftValue + ", right=" + rightValue, JoinWindows.of(Duration.ofSeconds(5))) .groupByKey() .reduce(((key, lastValue) -&amp;#62; lastValue)) .toStream() .to(OUTER_JOIN_STREAM_OUT_TOPIC); // build the streams topology final Topology topology = builder.build(); // TODO - 2: Add processor code later streamsOuterJoin = new KafkaStreams(topology, props); streamsOuterJoin.start(); } } &lt;/pre&gt; &lt;p&gt;When we do a join, we create a new value that combines the data in the left and right topics. If any record with a key is missing in the left or right topic, then the new value will have the string &lt;code&gt;null&lt;/code&gt; as the value for the missing record. Also, the Kafka Stream &lt;code&gt;reduce&lt;/code&gt; function returns the last-aggregated value for all of the keys.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;TODO 1 - Add state store&lt;/code&gt; and &lt;code&gt;TODO - 2:  Add processor code later&lt;/code&gt; comments are placeholders for code that we will add in the upcoming sections.&lt;/p&gt; &lt;h3&gt;The data flow so far&lt;/h3&gt; &lt;p&gt;Figure 4 illustrates the following data flow:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When a record with key A and value V1 comes into the left stream at time t1, Kafka Streams applies an outer join operation. At this point, the application creates a new record with key A and the value &lt;em&gt;left=V1, right=null&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;When a record with key A and value V2 arrives in the right topic, Kafka Streams again applies an outer join operation. This creates a new record with key A and the value &lt;em&gt;left=V1, right=V2&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;When the &lt;code&gt;reduce&lt;/code&gt; function is evaluated at the end of the duration window, the Kafka Streams API emits the last value that was computed, per the unique record key. In this case, it emits a record with key A and a value of &lt;em&gt;left=V1, right=V2&lt;/em&gt; into the new stream.&lt;/li&gt; &lt;li&gt;The new stream pushes the record to the &lt;code&gt;outerjoin&lt;/code&gt; topic.&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_786687" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-786687" class="wp-image-786687 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-1024x385.jpg" alt="A diagram of the data streaming pipeline." width="640" height="241" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-1024x385.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-300x113.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-768x289.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-786687" class="wp-caption-text"&gt;Figure 4: The data streaming pipeline so far.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, we will add the state store and processor code.&lt;/p&gt; &lt;h2&gt;Step 2: Add the Kafka Streams processor&lt;/h2&gt; &lt;p&gt;We need to process the records that are being pushed to the &lt;code&gt;outerjoin&lt;/code&gt; topic by the outer join operation. Kafka Streams provides a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html"&gt;Processor API&lt;/a&gt; that we can use to write custom logic for record processing. To start, we define a custom processor, &lt;code&gt;DataProcessor&lt;/code&gt;, and add it to the streams topology in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt;public class DataProcessor implements Processor&amp;#60;String, String&amp;#62;{ private ProcessorContext context; @Override public void init(ProcessorContext context) { this.context = context; } @Override public void process(String key, String value) { if(value.contains("null")) { // TODO 3: - let's process later } else { processRecord(key, value); //forward the processed data to processed-topic topic context.forward(key, value); } context.commit(); } @Override public void close() { } private void processRecord (String key, String value) { // your own custom logic. I just print System.out.println("==== Record Processed ==== key: "+key+" and value: "+value); } } &lt;/pre&gt; &lt;p&gt;The record is processed, and if the value does not contain a &lt;code&gt;null&lt;/code&gt; string, it is forwarded to the &lt;i&gt;sink&lt;/i&gt; topic (that is, the &lt;code&gt;processed-topic&lt;/code&gt; topic). In the bolded parts of the &lt;code&gt;KafkaStreaming&lt;/code&gt; class below, we wire the topology to define the source topic (i.e., the &lt;code&gt;outerjoin&lt;/code&gt; topic), add the processor, and finally add a sink (i.e., the &lt;code&gt;processed-topic&lt;/code&gt; topic). Once it&amp;#8217;s done, we can add this piece of code to the &lt;code&gt;TODO - 2: Add processor code later&lt;/code&gt; section of the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt;// add another stream that reads data from OUTER_JOIN_STREAM_OUT_TOPIC topic topology.addSource("Source", OUTER_JOIN_STREAM_OUT_TOPIC); // add a processor to the stream so that each record is processed topology.addProcessor("StateProcessor", new ProcessorSupplier&amp;#60;String, String&amp;#62;() { public Processor&amp;#60;String, String&amp;#62; get() { return new DataProcessor(); }}, "Source"); topology.addSink("Sink", PROCESSED_STREAM_OUT_TOPIC, "StateProcessor"); &lt;/pre&gt; &lt;p&gt;Note that all we do is to define the source topic (the &lt;code&gt;outerjoin&lt;/code&gt; topic), add an instance of our custom processor class, and then add the sink topic (the &lt;code&gt;processed-topic&lt;/code&gt; topic). The &lt;code&gt;context.forward()&lt;/code&gt; method in the custom processor sends the record to the sink topic.&lt;/p&gt; &lt;p&gt;Figure 5 shows the architecture that we have built so far.&lt;/p&gt; &lt;div id="attachment_786707" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor.jpg"&gt;&lt;img aria-describedby="caption-attachment-786707" class="wp-image-786707 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-1024x381.jpg" alt="A diagram of the architecture in progress." width="640" height="238" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-1024x381.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-300x112.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-768x286.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786707" class="wp-caption-text"&gt;Figure 5: The architecture with the Kafka Streams processor added.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Step 3: Add the punctuator and StateStore&lt;/h2&gt; &lt;p&gt;If you looked closely at the &lt;code&gt;DataProcessor&lt;/code&gt; class, you probably noticed that we are only processing records that have both of the required (left-stream and right-stream) key values. We also need to process records that have just one of the values, but we want to introduce a delay before processing these records. In some cases, the other value will arrive in a later time window, and we don&amp;#8217;t want to process the records prematurely.&lt;/p&gt; &lt;h3&gt;State store&lt;/h3&gt; &lt;p&gt;In order to delay processing, we need to hold incoming records in a store of some kind, rather than an external database. Kafka Streams lets us store data in a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html#state-stores"&gt;state store&lt;/a&gt;. We can use this type of store to hold recently received input records, track rolling aggregates, de-duplicate input records, and more.&lt;/p&gt; &lt;h3&gt;Punctuators&lt;/h3&gt; &lt;p&gt;Once we start holding records that have a missing value from either topic in a state store, we can use &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html#defining-a-stream-processor"&gt;punctuators&lt;/a&gt; to process them. As an example, we could add a &lt;code&gt;punctuator&lt;/code&gt; function to a &lt;code&gt;processorcontext.schedule()&lt;/code&gt; method. We can set the schedule to call the &lt;code&gt;punctuate()&lt;/code&gt; method.&lt;/p&gt; &lt;h3&gt;Add the state store&lt;/h3&gt; &lt;p&gt;Adding the following code to the &lt;code&gt;KafkaStreaming&lt;/code&gt; class adds a state store. Place this code where you see the &lt;code&gt;TODO 1 - Add state store&lt;/code&gt; comment in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; // build the state store that will eventually store all unprocessed items Map&amp;#60;String, String&amp;#62; changelogConfig = newHashMap&amp;#60;&amp;#62;(); StoreBuilder&amp;#60;KeyValueStore&amp;#60;String, String&amp;#62;&amp;#62; stateStore = Stores.keyValueStoreBuilder( Stores.persistentKeyValueStore(STORE_NAME), Serdes.String(), Serdes.String()) .withLoggingEnabled(changelogConfig); ..... ..... ..... ..... // add the state store in the topology builder topology.addStateStore(stateStore, "StateProcessor"); &lt;/pre&gt; &lt;p&gt;We have defined a state store that stores the key and value as a string. We&amp;#8217;ve also enabled logging, which is useful if the application dies and restarts. In that case, the state store won&amp;#8217;t lose data.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll modify the processor&amp;#8217;s &lt;code&gt;process()&lt;/code&gt; to put records with a missing value from either topic in the state store for later processing. Place the following code where you see the comment &lt;code&gt;TODO 3 - let's process later&lt;/code&gt; in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; if(value.contains("null")) { if (kvStore.get(key) != null) { // this means that the other value arrived first // you have both the values now and can process the record String newvalue = value.concat(" ").concat(kvStore.get(key)); process(key, newvalue); // remove the entry from the statestore (if any left or right record came first as an event) kvStore.delete(key); context.forward(key, newvalue); } else { // add to state store as either left or right data is missing System.out.println("Incomplete value: "+value+" detected. Putting into statestore for later processing"); kvStore.put(key, value); } } &lt;/pre&gt; &lt;h3&gt;Add the punctuator&lt;/h3&gt; &lt;p&gt;Next, we add the punctuator to the custom processor we&amp;#8217;ve just created. For this, we update the &lt;code&gt;DataProcessor&lt;/code&gt;&amp;#8216;s &lt;code&gt;init()&lt;/code&gt; method to the following:&lt;/p&gt; &lt;pre&gt; private KeyValueStore&amp;#60;String, String&amp;#62; kvStore; @Override public void init(ProcessorContext context) { this.context = context; kvStore = (KeyValueStore) context.getStateStore(STORE_NAME); // schedule a punctuate() method every 50 seconds based on stream-time this.context.schedule(Duration.ofSeconds(50), PunctuationType.WALL_CLOCK_TIME, new Punctuator(){ @Override public void punctuate(long timestamp) { System.out.println("Scheduled punctuator called at "+timestamp); KeyValueIterator&amp;#60;String, String&amp;#62; iter = kvStore.all(); while (iter.hasNext()) { KeyValue&amp;#60;String, String&amp;#62; entry = iter.next(); System.out.println(" Processed key: "+entry.key+" and value: "+entry.value+" and sending to processed-topic topic"); context.forward(entry.key, entry.value.toString()); kvStore.put(entry.key, null); } iter.close(); // commit the current processing progress context.commit(); } } ); } &lt;/pre&gt; &lt;p&gt;We&amp;#8217;ve set the punctuate logic to be invoked every 50 seconds. The code retrieves entries in the state store and processes them. The &lt;code&gt;forward()&lt;/code&gt; function then sends the processed record to the &lt;code&gt;processed-topic&lt;/code&gt; topic. Lastly, we delete the record from the state store.&lt;/p&gt; &lt;p&gt;Figure 6 shows the complete data streaming architecture:&lt;/p&gt; &lt;div id="attachment_786717" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture.jpg"&gt;&lt;img aria-describedby="caption-attachment-786717" class="wp-image-786717 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-1024x390.jpg" alt="A diagram of the complete application with the state store and punctuators added." width="640" height="244" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-1024x390.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-300x114.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-768x293.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786717" class="wp-caption-text"&gt;Figure 6: The complete data streaming pipeline.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Interactive queries&lt;/h2&gt; &lt;p&gt;We are finished with the basic data streaming pipeline, but what if we wanted to be able to query the state store? In this case, we could use &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/interactive-queries.html"&gt;interactive queries&lt;/a&gt; in the Kafka Streams API to make the application queryable. See the article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus/blob/master/quarkus-kafka-streaming/src/main/java/org/acme/InteractiveQueries.java"&gt;GitHub repository&lt;/a&gt; for more about interactive queries in Kafka Streams.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;You can use the streaming pipeline that we developed in this article to do any of the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Process records in real-time.&lt;/li&gt; &lt;li&gt;Store data without depending on a database or cache.&lt;/li&gt; &lt;li&gt;Build a modern, &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hope the example application and instructions will help you with building and processing data streaming pipelines. You can get the source code for the example application from this article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#038;title=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" data-a2a-url="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/" data-a2a-title="Build a data streaming pipeline using Kafka Streams and Quarkus"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/"&gt;Build a data streaming pipeline using Kafka Streams and Quarkus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6-PYLEG5zIc" height="1" width="1" alt=""/&gt;</content><summary>In typical data warehousing systems, data is first accumulated and then processed. But with the advent of new technologies, it is now possible to process data as and when it arrives. We call this real-time data processing. In real-time processing, data streams through pipelines; i.e., moving from one system to another. Data gets generated from static sources (like databases) or real-time systems (...</summary><dc:creator>Kapil Shukla</dc:creator><dc:date>2020-09-28T07:00:22Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/</feedburner:origLink></entry><entry><title>How to setup OpenShift Container Platform 4.5 on your local machine in minutes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zFfVGIyo8UA/how-to-setup-openshift-container-platform-45.html" /><category term="cloud" scheme="searchisko:content:tags" /><category term="CodeReadyContainers" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-how_to_setup_openshift_container_platform_4_5_on_your_local_machine_in_minutes</id><updated>2020-09-28T05:00:05Z</updated><published>2020-09-28T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-ezsrnZBkM34/X2IOwlzFutI/AAAAAAAAxi0/sbIfjQl1aH8_W9fJW5fccdT26o2ka92bQCNcBGAsYHQ/s1600/kevin-ku-w7ZyuGYNpRQ-unsplash.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img alt="CodeReady Containers" border="0" data-original-height="1200" data-original-width="1600" height="240" src="https://1.bp.blogspot.com/-ezsrnZBkM34/X2IOwlzFutI/AAAAAAAAxi0/sbIfjQl1aH8_W9fJW5fccdT26o2ka92bQCNcBGAsYHQ/s320/kevin-ku-w7ZyuGYNpRQ-unsplash.jpg" title="" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform?&lt;br /&gt;&lt;br /&gt;Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.5?&lt;br /&gt;&lt;br /&gt;Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud native development and automated rolling deployments. Since I started pulling together ways to easily experience this with OpenShift Container Platform, back with&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;version 3.3&lt;/a&gt;&amp;nbsp;believe it or not, we've come a long ways.&lt;br /&gt;&lt;br /&gt;The idea was to make this as streamlined of an experience as possible by using the same&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;CodeReady Containers Easy Install project&lt;/a&gt;. Let's take a look at what this looks like.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;The first focus was to have this work for Unix based operating systems using a single installation script. The secondary wish is to provide a windows based installation script.&lt;br /&gt;&lt;br /&gt;&lt;h3 data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}"&gt;Linux or Mac installation&lt;/h3&gt;This installation requires the following (all freely available):&lt;br /&gt;&lt;br /&gt;&lt;pre class="code highlight" lang="plaintext"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&lt;span class="line" data-keep-original-tag="false" id="LC1" lang="plaintext"&gt;1. HyperKit for OSX, Hyper-V for Windows, or Libvirt for Linux&lt;/span&gt;&lt;br /&gt;&lt;span class="line" data-keep-original-tag="false" id="LC2" lang="plaintext"&gt;2. Code Ready Containers (OCP 4.5)&lt;/span&gt;&lt;br /&gt;&lt;span class="line" data-keep-original-tag="false" id="LC3" lang="plaintext"&gt;3. OpenShift Client (oc) v4.5&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;br /&gt;First you need to ensure your virtualization tooling is installed for your platform, just search online for how to do that or your specific platform. Second you need to download the CodeReady Containers. Finally, you need the OpenShift client. Normally you'd expect to have to track these last two down but we've made this all easy by just including checks during the installation. If you have something installed, it checks the version, if good then it moves on with next steps. If anything is missing or wrong version, the installation stops and notifies you where to find that component for your platform (including URL).&lt;br /&gt;&lt;br /&gt;Let's get started by downloading the&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/ocp-install-demo/-/archive/master/ocp-install-demo-master.zip" target="_blank"&gt;CodeReady Containers Easy Install&lt;/a&gt; project and unzipping in some directory. This gives you a file called&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;ocp-install-demo-master.zip&lt;/span&gt;,&lt;/span&gt;just unzip and run the&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;init.sh&lt;/span&gt;&lt;/span&gt;&amp;nbsp;as follows:&lt;br /&gt;&lt;br /&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $ ./init.sh&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Follow the instructions as each of the dependencies is checked and you're provided with pointers to getting the versions you need for your platform.&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Note: Each CodeReady Container download is tied to an embedded secret. This secret you need to download (link will be provided) as a file and you'll be asked to point to that secret to start your container platform.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Once you've gotten all the dependencies sorted out, the install runs like this:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-Zam6j6uD34w/X2H3oCplT_I/AAAAAAAAxhE/0nu75cf7gFgUEkHRB2zMByyDJb-zo9tTQCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B13.31.31.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="844" data-original-width="792" height="640" src="https://1.bp.blogspot.com/-Zam6j6uD34w/X2H3oCplT_I/AAAAAAAAxhE/0nu75cf7gFgUEkHRB2zMByyDJb-zo9tTQCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B13.31.31.png" width="600" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;A little ASCII art and then it's checking for my platforms virtualization (Hyperkit), then looking for the OpenShift client version 4.5 (oc client), then running a setup (crc setup).&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-1Mhlckwprew/X2H35HT9KSI/AAAAAAAAxhM/HSuSwsFWuKwlNjC_gSBhEJYAYk30h6wvgCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B13.32.42.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="406" data-original-width="813" height="316" src="https://1.bp.blogspot.com/-1Mhlckwprew/X2H35HT9KSI/AAAAAAAAxhM/HSuSwsFWuKwlNjC_gSBhEJYAYk30h6wvgCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B13.32.42.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;The next steps are providing the pull-secret-file, you can set this in the variables at the top of the installation script. Now the moment of truth, the CodeReady Containers cluster starts, which takes some time depending on your network (crc start). With a good networks it's about a five minute wait.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-oOe-GagsJAI/X2IBlVKuYDI/AAAAAAAAxhY/XULsOy81nYsukh9gv4CJLqZchcc2DZZ4QCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.07.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="578" data-original-width="978" height="378" src="https://1.bp.blogspot.com/-oOe-GagsJAI/X2IBlVKuYDI/AAAAAAAAxhY/XULsOy81nYsukh9gv4CJLqZchcc2DZZ4QCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.07.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;This is the logging you'll see as the OpenShift cluster starts on your local machine. The warning is normal, just some of the features have been trimmed to speed up deployment.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-9vrBZLMiS64/X2IButB2LYI/AAAAAAAAxhg/7tqEa0FaATgjB9cK6t0UHixRBoHooCfgwCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.46.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="259" data-original-width="780" height="212" src="https://1.bp.blogspot.com/-9vrBZLMiS64/X2IButB2LYI/AAAAAAAAxhg/7tqEa0FaATgjB9cK6t0UHixRBoHooCfgwCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.46.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-6CAagw6bs7c/X2IBuranlVI/AAAAAAAAxhc/1pdEOE9pqvswXZ2FGAGOs48QJ3Gqo4A_ACNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.57.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="461" data-original-width="553" height="531" src="https://1.bp.blogspot.com/-6CAagw6bs7c/X2IBuranlVI/AAAAAAAAxhc/1pdEOE9pqvswXZ2FGAGOs48QJ3Gqo4A_ACNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.57.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;At the end we'll retrieve the admin password for logging in to the cluster's console, pick up the host URL, test the deployment by logging in with our client (oc login), and finally you're given all the details in a nice box. You have the option to stop, start it again, or delete the OpenShift Container Platform cluster as shown in the dialog.&lt;br /&gt;&lt;br /&gt;Next open the web console using URL and login 'kubeadmin' with the corresponding password. In our case it's the URL:&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace; font-size: x-small;"&gt;https://console-openshift-console.apps-crc.testing&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/--B9bC4IjKGc/X2ICGZjji4I/AAAAAAAAxhw/7og6eJ6RKIsu2hRqHHQsDfrtt0fckNOkACNcBGAsYHQ/s1600/ocp-login.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="804" data-original-width="1600" height="320" src="https://1.bp.blogspot.com/--B9bC4IjKGc/X2ICGZjji4I/AAAAAAAAxhw/7og6eJ6RKIsu2hRqHHQsDfrtt0fckNOkACNcBGAsYHQ/s640/ocp-login.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;Log in with user:&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;kubeadmin&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;Password in our case:&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace; font-size: x-small;"&gt;duduw-yPT9Z-hsUpq-f3pre&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;That opens the main dashboard:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-guFcKoTIsR4/X2ICTPQJdNI/AAAAAAAAxh0/eK5qAyCYrAEbc3lZhZysU2MMs5Xh_eEOwCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.17.04.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="898" data-original-width="1534" height="374" src="https://1.bp.blogspot.com/-guFcKoTIsR4/X2ICTPQJdNI/AAAAAAAAxh0/eK5qAyCYrAEbc3lZhZysU2MMs5Xh_eEOwCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.17.04.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;Verify the version you are running by clicking on the top right question mark and then &lt;i&gt;About&lt;/i&gt;&amp;nbsp;option:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-1Y0cF0HHgJQ/X2ICqMAgeFI/AAAAAAAAxiE/D76GzK_OxmYAcb6xDdYIoXi-OhYaNUKwgCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.11.54.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="900" data-original-width="1533" height="374" src="https://1.bp.blogspot.com/-1Y0cF0HHgJQ/X2ICqMAgeFI/AAAAAAAAxiE/D76GzK_OxmYAcb6xDdYIoXi-OhYaNUKwgCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.11.54.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Close the version window by clicking on the X. As we are interested in developing using the tooling and container images provided by CodeReady Containers, let's change the view from &lt;i&gt;Administrator &lt;/i&gt;to &lt;i&gt;Developer &lt;/i&gt;in the left top menu selecting &lt;i&gt;Topology &lt;/i&gt;and then via &lt;i&gt;Project&lt;/i&gt;&amp;nbsp;drop down menu at the top choose &lt;i&gt;Default&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-cZNIzNMZboE/X2IDYdKmFjI/AAAAAAAAxic/Q-fD4xSoAqk2TtfzLwY3uY31FnCGkPh1ACNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.20.53.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="899" data-original-width="1537" height="374" src="https://1.bp.blogspot.com/-cZNIzNMZboE/X2IDYdKmFjI/AAAAAAAAxic/Q-fD4xSoAqk2TtfzLwY3uY31FnCGkPh1ACNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.20.53.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;You can browse the offerings in the provided container catalog by selecting &lt;i&gt;From Catalog&lt;/i&gt;&amp;nbsp;and then for example, &lt;i&gt;Middleware &lt;/i&gt;to view the offerings available:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-0F8QRn_Ustw/X2IDxZDFg0I/AAAAAAAAxio/Ewhn9ClT6Cc3J73Lb2Qc3sPimreZFYKpQCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.22.57.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="826" data-original-width="1536" height="344" src="https://1.bp.blogspot.com/-0F8QRn_Ustw/X2IDxZDFg0I/AAAAAAAAxio/Ewhn9ClT6Cc3J73Lb2Qc3sPimreZFYKpQCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.22.57.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Looking to get started with an example usage, try the &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rhpam-install-demo" target="_blank"&gt;Red Hat Process Automation Manager&lt;/a&gt; or &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rhdm-install-demo" target="_blank"&gt;Red Hat Decision Manager &lt;/a&gt;examples that leverage the provided developer catalog container images. You can also explore how an existing project is setup using one of the developer catalog container images with a &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rewards-demo" target="_blank"&gt;human resources employee rewards project&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;This concludes the installation and tour of an OpenShift Container Platform on our local machine using CodeReady Containers.&lt;br /&gt;&lt;br /&gt;&lt;h3 data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}"&gt;What about Windows?&lt;/h3&gt;If you are a sharp observer, you'll notice there is a file called&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo/blob/master/init.bat&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;init.bat&lt;/span&gt;&lt;/a&gt;&amp;nbsp;for windows platforms to install with. The problem is I've not been able to test this yet on a windows machine, so I'd love to call out to the readers out there that might have some time to contribute to test this script and help us complete the installation. You'll notice a few TODO's marked in the scripts code, as they are untested areas in the installation.&lt;br /&gt;&lt;br /&gt;You can&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo/issues/new?issue%5Bassignee_id%5D=&amp;amp;issue%5Bmilestone_id%5D=&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;raise new issues here&lt;/a&gt;&amp;nbsp;and help us complete the windows based installation and get your name added to the contributors list. We'd be really thankful!&lt;br /&gt;&lt;br /&gt;Stay tuned for more on cloud-native development using other Red Hat technologies on your new OpenShift Container Platform installed locally on your own machine!&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/n_b1jiOlhGY" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zFfVGIyo8UA" height="1" width="1" alt=""/&gt;</content><summary>Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform? Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.5? Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2020-09-28T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/n_b1jiOlhGY/how-to-setup-openshift-container-platform-45.html</feedburner:origLink></entry><entry><title>Rootless containers with Podman: The basics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qM0ia1DXS7w/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="linux" scheme="searchisko:content:tags" /><category term="oci image" scheme="searchisko:content:tags" /><category term="open source" scheme="searchisko:content:tags" /><category term="Podman" scheme="searchisko:content:tags" /><category term="RHEL 7" scheme="searchisko:content:tags" /><category term="rhel 8" scheme="searchisko:content:tags" /><category term="rootless containers" scheme="searchisko:content:tags" /><category term="rootless podman" scheme="searchisko:content:tags" /><category term="Universal Base Images (UBI)" scheme="searchisko:content:tags" /><author><name>Prakhar Sethi</name></author><id>searchisko:content:id:jbossorg_blog-rootless_containers_with_podman_the_basics</id><updated>2020-09-25T07:00:56Z</updated><published>2020-09-25T07:00:56Z</published><content type="html">&lt;p&gt;As a developer, you have probably heard a lot about containers. A &lt;a href="https://developers.redhat.com/topics/containers"&gt;&lt;i&gt;container&lt;/i&gt;&lt;/a&gt; is a unit of software that provides a packaging mechanism that abstracts the code and all of its dependencies to make application builds fast and reliable. An easy way to experiment with containers is with the Pod Manager tool (&lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools"&gt;Podman&lt;/a&gt;), which is a daemonless, open source, &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;-native tool that provides a command-line interface (CLI) similar to the docker container engine.&lt;/p&gt; &lt;p&gt;In this article, I will explain the benefits of using containers and Podman, introduce rootless containers and why they are important, and then show you how to use rootless containers with Podman with an example. Before we dive into the implementation, let&amp;#8217;s review the basics.&lt;/p&gt; &lt;p&gt;&lt;span id="more-748207"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Why containers?&lt;/h2&gt; &lt;p&gt;Using containers isolates your applications from the various computing environments in which they run. They have become increasingly popular because they help developers focus on the application logic and its dependencies, which they bind in a single unit. Operations teams also like containers because they can focus on managing the application, including deployment, without bothering with details such as software versions and configuration.&lt;/p&gt; &lt;p&gt;Containers virtualize at the operating system (OS) level. This makes them lightweight, unlike virtual machines, which virtualize at the hardware level. In a nutshell, here are the advantages of using containers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Low hardware footprint&lt;/li&gt; &lt;li&gt;Environment isolation&lt;/li&gt; &lt;li&gt;Quick deployment&lt;/li&gt; &lt;li&gt;Multiple environment deployments&lt;/li&gt; &lt;li&gt;Reusability&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why Podman?&lt;/h2&gt; &lt;p&gt;Using Podman makes it easy to find, run, build, share, and deploy applications using &lt;a target="_blank" rel="nofollow" href="https://opencontainers.org/"&gt;Open Container Initiative&lt;/a&gt; (OCI)-compatible containers and container images. Podman&amp;#8217;s advantages are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It is &lt;em&gt;daemonless&lt;/em&gt;; it does not require a daemon, unlike docker.&lt;/li&gt; &lt;li&gt;It lets you control the layers of the container; sometimes, you want a single layer, and sometimes you need 12 layers.&lt;/li&gt; &lt;li&gt;It uses the fork/exec model for containers instead of the client/server model.&lt;/li&gt; &lt;li&gt;It lets you run containers as a non-root user, so you never have to give a user root permission on the host. This obviously differs from the client/server model, where you must open a socket to a privileged daemon running as root to launch a container.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why rootless containers?&lt;/h2&gt; &lt;p&gt;&lt;i&gt;Rootless containers&lt;/i&gt; are containers that can be created, run, and managed by users without admin rights. Rootless containers have several advantages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They add a new security layer; even if the container engine, runtime, or orchestrator is compromised, the attacker won&amp;#8217;t gain root privileges on the host.&lt;/li&gt; &lt;li&gt;They allow multiple unprivileged users to run containers on the same machine (this is especially advantageous in &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/podman-paves-road-running-containerized-hpc-applications-exascale-supercomputers"&gt;high-performance computing environments&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;They allow for isolation inside of nested containers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To better understand these advantages, consider traditional resource management and scheduling systems. This type of system should be run by unprivileged users. From a security perspective, fewer privileges are better. With rootless containers, you can run a containerized process as any other process without needing to escalate any user&amp;#8217;s privileges. There is no daemon; Podman creates a child process.&lt;/p&gt; &lt;h2&gt;Example: Using rootless containers&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s get started using rootless containers with Podman. We&amp;#8217;ll start with the basic setup and configuration.&lt;/p&gt; &lt;h3&gt;System requirements&lt;/h3&gt; &lt;p&gt;We will need &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 7.7 or greater for this implementation. Assuming you have that, we can begin configuring the example.&lt;/p&gt; &lt;h3&gt;Configuration&lt;/h3&gt; &lt;p&gt;First, install &lt;code&gt;slirp4netns&lt;/code&gt; and Podman on your machine by entering the following command:&lt;/p&gt; &lt;pre&gt;$ yum install slirp4netns podman -y &lt;/pre&gt; &lt;p&gt;We will use &lt;code&gt;slirp4netns&lt;/code&gt; to connect a network namespace to the internet in a completely rootless (or unprivileged) way.&lt;/p&gt; &lt;p&gt;When the installation is done, increase the number of user namespaces. Use the following commands :&lt;/p&gt; &lt;pre&gt;$ echo “user.max_user_namespaces=28633” &amp;#62; /etc/sysctl.d/userns.conf $ sysctl -p /etc/sysctl.d/userns.conf &lt;/pre&gt; &lt;p&gt;Next, create a new user account and name it. In this case, my user account is named Red Hat:&lt;/p&gt; &lt;pre&gt;$ useradd -c “Red Hat” redhat &lt;/pre&gt; &lt;p&gt;Use the following command to set the password for the new account (note that you must insert your own password):&lt;/p&gt; &lt;pre&gt;$ passwd redhat &lt;/pre&gt; &lt;p&gt;This user is now automatically configured to be able to use a rootless instance of Podman.&lt;/p&gt; &lt;h3&gt;Connect to the user&lt;/h3&gt; &lt;p&gt;Now, try running a Podman command as the user you&amp;#8217;ve just created.  Do not use &lt;code&gt;su -&lt;/code&gt; because that command doesn&amp;#8217;t set the correct environment variables. Instead, you can use any other command to connect to that user. Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;$ ssh redhat@localhost &lt;/pre&gt; &lt;h3&gt;Pull a Red Hat Enterprise Linux image&lt;/h3&gt; &lt;p&gt;After logging in, try pulling a RHEL image using the &lt;code&gt;podman&lt;/code&gt; command (note that &lt;code&gt;ubi&lt;/code&gt; stands for &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Image&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;$ podman pull ubi7/ubi &lt;/pre&gt; &lt;p&gt;If you want more information about the image, run this command:&lt;/p&gt; &lt;pre&gt;$ podman run ubi7/ubi cat /etc/os-release &lt;/pre&gt; &lt;p&gt;To check the images that resulted from the above command, along with any other images on your system, run the command:&lt;/p&gt; &lt;pre&gt;$ podman images &lt;/pre&gt; &lt;p&gt;It is also possible for a rootless user to create a container from these images, but I&amp;#8217;ll save that for another article.&lt;/p&gt; &lt;h3&gt;Check the rootless configuration&lt;/h3&gt; &lt;p&gt;Finally, verify whether your rootless configuration is properly set up. Run the following command to show how the UIDs are assigned to the user namespace:&lt;/p&gt; &lt;pre&gt;$ podman unshare cat /proc/self/uid_map &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article demonstrated how to set up rootless containers with Podman. Here are some tips for working with rootless containers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As a non-root container user, container images are stored under your home directory (for instance, &lt;code&gt;$HOME/.local/share/containers/storage&lt;/code&gt;), instead of &lt;code&gt;/var/lib/containers&lt;/code&gt;. This directory scheme ensures that you have enough storage for your home directory.&lt;/li&gt; &lt;li&gt;Users running rootless containers are given special permission to run on the host system using a range of user and group IDs. Otherwise, they have no root privileges to the operating system on the host.&lt;/li&gt; &lt;li&gt;A container running as root in a rootless account can turn on privileged features within its own namespace. But that doesn&amp;#8217;t provide any special privileges to access protected features on the host (beyond having extra UIDs and GIDs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Learn more about &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/index#set_up_for_rootless_containers"&gt;setting up rootless containers with Podman here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#038;title=Rootless%20containers%20with%20Podman%3A%20The%20basics" data-a2a-url="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/" data-a2a-title="Rootless containers with Podman: The basics"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/"&gt;Rootless containers with Podman: The basics&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qM0ia1DXS7w" height="1" width="1" alt=""/&gt;</content><summary>As a developer, you have probably heard a lot about containers. A container is a unit of software that provides a packaging mechanism that abstracts the code and all of its dependencies to make application builds fast and reliable. An easy way to experiment with containers is with the Pod Manager tool (Podman), which is a daemonless, open source, Linux-native tool that provides a command-line inte...</summary><dc:creator>Prakhar Sethi</dc:creator><dc:date>2020-09-25T07:00:56Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/</feedburner:origLink></entry><entry><title>New C++ features in GCC 10</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JYKdxbvy15s/" /><category term="C++" scheme="searchisko:content:tags" /><category term="C++20" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="GCC 10" scheme="searchisko:content:tags" /><category term="linux" scheme="searchisko:content:tags" /><category term="open source" scheme="searchisko:content:tags" /><category term="Programming Languages" scheme="searchisko:content:tags" /><category term="RHEL 7" scheme="searchisko:content:tags" /><category term="rhel 8" scheme="searchisko:content:tags" /><author><name>Marek Polacek</name></author><id>searchisko:content:id:jbossorg_blog-new_c_features_in_gcc_10</id><updated>2020-09-24T07:00:32Z</updated><published>2020-09-24T07:00:32Z</published><content type="html">&lt;p&gt;The GNU Compiler Collection (GCC) 10.1 was released in May 2020. Like every other GCC release, this version brought many &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/gcc-10/changes.html"&gt;additions, improvements, bug fixes, and new features&lt;/a&gt;. Fedora 32 already ships GCC 10 as the system compiler, but it&amp;#8217;s also possible to try GCC 10 on other platforms (see &lt;a target="_blank" rel="nofollow" href="https://godbolt.org/"&gt;godbolt.org&lt;/a&gt;, for example). &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) users will get GCC 10 in the Red Hat Developer Toolset (RHEL 7), or the Red Hat GCC Toolset (RHEL 8).&lt;/p&gt; &lt;p&gt;This article focuses on the part of the GCC compiler on which I spend most of my time: The C++ front end. My goal is to present new features that might be of interest to C++ application programmers. Note that I do not discuss developments in the C++ language itself, although some language updates overlap with compiler updates. I also do not discuss changes in the standard C++ library that comes with GCC 10.&lt;/p&gt; &lt;p&gt;We implemented many C++20 proposals in GCC 10. For the sake of brevity, I won&amp;#8217;t describe them in great detail. The default dialect in GCC 10 is &lt;code&gt;-std=gnu++14&lt;/code&gt;; to enable C++20 features, use the &lt;code&gt;-std=c++20&lt;/code&gt; or &lt;code&gt;-std=gnu++20&lt;/code&gt; command-line option. (Note that the latter option allows GNU extensions.)&lt;/p&gt; &lt;h2&gt;C++ concepts&lt;/h2&gt; &lt;p&gt;While previous versions of GCC (GCC 6 was the first) had initial implementations of &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0734r0.pdf"&gt;C++ concepts&lt;/a&gt;, GCC 10 updated concepts to conform to the C++20 specification. This update also improved compile times. Subsequent patches have improved concepts-related diagnostics.&lt;/p&gt; &lt;p&gt;In C++ template parameters, &lt;code&gt;typename&lt;/code&gt; means any type. But most templates must be constrained in some way; as an example, you want to only accept types that have certain properties, not just any type. Failing to use the correct type often results in awful and verbose error messages. In C++20, you can constrain a type by using a &lt;em&gt;concept&lt;/em&gt;, which is a compile-time predicate that specifies the set of operations that can be applied to the type. Using GCC 10, you can define your own concept (or use one defined in &lt;code&gt;&amp;#60;concepts&amp;#62;&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;#include &amp;#60;type_traits&amp;#62; // Require that T be an integral type. template&amp;#60;typename T&amp;#62; concept C = std::is_integral_v&amp;#60;T&amp;#62;; &lt;/pre&gt; &lt;p&gt;And then use it like this:&lt;/p&gt; &lt;pre&gt;template&amp;#60;C T&amp;#62; void f(T) { } void g () { f (1); // OK f (1.2); // error: use of function with unsatisfied constraints } &lt;/pre&gt; &lt;p&gt;Starting with GCC 10, the C++ compiler also supports &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1141r2.html"&gt;constrained auto&lt;/a&gt;. Using our concept above, you can now write:&lt;/p&gt; &lt;pre&gt;int fn1 (); double fn2 (); void h () { C auto x1 = fn1 (); // OK C auto x2 = fn2 (); // error: deduced initializer does not satisfy placeholder constraints }&lt;/pre&gt; &lt;h2&gt;Coroutines&lt;/h2&gt; &lt;p&gt;GCC 10 supports &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0912r5.html"&gt;stackless functions&lt;/a&gt; that can be suspended and resumed later without losing their state. This feature lets us execute sequential code asynchronously. It requires the &lt;code&gt;-fcoroutines&lt;/code&gt; command-line option.&lt;/p&gt; &lt;h3&gt;Unevaluated inline-assembly in &lt;code&gt;constexpr&lt;/code&gt; functions&lt;/h3&gt; &lt;p&gt;Code like this now compiles:&lt;/p&gt; &lt;pre&gt;constexpr int foo (int a, int b) { if (std::is_constant_evaluated ()) return a + b; // Not in a constexpr context. asm ("/* assembly */"); return a; }&lt;/pre&gt; &lt;p&gt;See &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1668r1.html"&gt;the proposal&lt;/a&gt; for details.&lt;/p&gt; &lt;h3&gt;Comma expression in array subscript expressions&lt;/h3&gt; &lt;p&gt;This type of expression is now &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1161r3.html"&gt;deprecated&lt;/a&gt;, so GCC 10 warns for code like this:&lt;/p&gt; &lt;pre&gt;int f (int arr[], int a, int b) { return arr[a, b]; } &lt;/pre&gt; &lt;p&gt;Only a top-level comma is deprecated, however, so &lt;code&gt;arr[(a, b)]&lt;/code&gt; compiles without a warning.&lt;/p&gt; &lt;h3&gt;Structured bindings&lt;/h3&gt; &lt;p&gt;GCC 10 improves and &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1091r3.html"&gt;extends&lt;/a&gt; structured bindings. For instance, it&amp;#8217;s now possible to mark them &lt;code&gt;static&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;struct S { int a, b, c; } s; static auto [ x, y, z ] = s; &lt;/pre&gt; &lt;p&gt;This example doesn&amp;#8217;t compile with GCC 9, but it compiles with GCC 10.&lt;/p&gt; &lt;h2&gt;The &lt;code&gt;constinit&lt;/code&gt; keyword&lt;/h2&gt; &lt;p&gt;GCC 10 uses the &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1143r2.html"&gt;C++20 specifier&lt;/a&gt; &lt;code&gt;constinit&lt;/code&gt; to ensure that a (static storage duration) variable is initialized by a constant initializer.&lt;/p&gt; &lt;p&gt;This might alleviate problems with the &lt;em&gt;static initialization order fiasco&lt;/em&gt;. However, the variable is not constant: It is possible to modify it after initialization has taken place. Consider:&lt;/p&gt; &lt;pre&gt;constexpr int fn1 () { return 42; } int fn2 () { return -1; } constinit int i1 = fn1 (); // OK constinit int i2 = fn2 (); // error: constinit variable does not have a constant initializer &lt;/pre&gt; &lt;p&gt;GCC doesn&amp;#8217;t support Clang&amp;#8217;s &lt;code&gt;require_constant_initialization&lt;/code&gt; attribute, so you can use &lt;code&gt;__constinit&lt;/code&gt; in earlier C++ modes, as an extension, to get a similar effect.&lt;/p&gt; &lt;h2&gt;Deprecated uses of &lt;code&gt;volatile&lt;/code&gt;&lt;/h2&gt; &lt;p&gt;Expressions that involve both loads and stores of a &lt;code&gt;volatile&lt;/code&gt; lvalue, such as &lt;code&gt;++&lt;/code&gt; or &lt;code&gt;+=&lt;/code&gt;, are &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1152r4.html"&gt;deprecated&lt;/a&gt;. The &lt;code&gt;volatile&lt;/code&gt;-qualified parameter and return types are also deprecated, so GCC 10 will warn in:&lt;/p&gt; &lt;pre&gt;void fn () { volatile int v = 42; // Load + store or just a load? ++v; // warning: deprecated } &lt;/pre&gt; &lt;h3&gt;Conversions to arrays of unknown bound&lt;/h3&gt; &lt;p&gt;Converting to an array of unknown bound is now &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0388r4.html"&gt;permitted&lt;/a&gt;, so the following code compiles:&lt;/p&gt; &lt;pre&gt;void f(int(&amp;#38;)[]); int arr[1]; void g() { f(arr); } int(&amp;#38;r)[] = arr; &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: Conversion in the other direction—&lt;em&gt;from&lt;/em&gt; arrays of unknown bound—currently is not allowed by the C++ standard.&lt;/p&gt; &lt;h3&gt;&lt;code&gt;constexpr new&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;This &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0784r7.html"&gt;feature&lt;/a&gt; allows dynamic memory allocation at compile time in a &lt;code&gt;constexpr&lt;/code&gt; context:&lt;/p&gt; &lt;pre&gt;constexpr auto fn () { int *p = new int{10}; // ... use p ... delete p; return 0; } int main () { constexpr auto i = fn (); } &lt;/pre&gt; &lt;p&gt;Note that the storage allocated at compile time in a &lt;code&gt;constexpr&lt;/code&gt; context must also be freed at compile time. And, given that &lt;code&gt;constexpr&lt;/code&gt; doesn&amp;#8217;t allow undefined behavior, &lt;code&gt;use-after-free&lt;/code&gt; is a compile-time error. The &lt;code&gt;new&lt;/code&gt; expression also can&amp;#8217;t throw. This feature paves the way for &lt;code&gt;constexpr&lt;/code&gt; standard containers such as &lt;code&gt;&amp;#60;vector&amp;#62;&lt;/code&gt; and &lt;code&gt;&amp;#60;string&amp;#62;&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;The &lt;code&gt;[[nodiscard]]&lt;/code&gt; attribute&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;[[nodiscard]]&lt;/code&gt; attribute now supports an optional argument, like so:&lt;/p&gt; &lt;pre&gt;[[nodiscard("unsafe")]] int *fn (); &lt;/pre&gt; &lt;p&gt;See &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1301r4.html"&gt;the proposal&lt;/a&gt; for details.&lt;/p&gt; &lt;h2&gt;CTAD extensions&lt;/h2&gt; &lt;p&gt;C++20 class template argument deduction (CTAD) now works for &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1814r0.html"&gt;alias templates&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1816r0.pdf"&gt;aggregates&lt;/a&gt;, too:&lt;/p&gt; &lt;pre&gt;template &amp;#60;typename T&amp;#62; struct Aggr { T x; }; Aggr a = { 1 }; // works in GCC 10 template &amp;#60;typename T&amp;#62; struct NonAggr { NonAggr(); T x; }; NonAggr n = { 1 }; // error: deduction fails &lt;/pre&gt; &lt;h2&gt;Parenthesized initialization of aggregates&lt;/h2&gt; &lt;p&gt;You can now initialize an aggregate using a &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0960r3.html"&gt;parenthesized list of values&lt;/a&gt; such as &lt;code&gt;(1, 2, 3)&lt;/code&gt;. The behavior is similar to &lt;code&gt;{1, 2, 3}&lt;/code&gt;, but in parenthesized initialization, the following exceptions apply:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Narrowing conversions are permitted.&lt;/li&gt; &lt;li&gt;Designators (things like &lt;code&gt;.a = 10&lt;/code&gt;) are not permitted.&lt;/li&gt; &lt;li&gt;A temporary object bound to a reference does not have its lifetime extended.&lt;/li&gt; &lt;li&gt;There is no brace elision.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;struct A { int a, b; }; A a1{1, 2}; A a2(1, 2); // OK in GCC 10 -std=c++20 auto a3 = new A(1, 2); // OK in GCC 10 -std=c++20 &lt;/pre&gt; &lt;h3&gt;Trivial default initialization in &lt;code&gt;constexpr&lt;/code&gt; contexts&lt;/h3&gt; &lt;p&gt;This usage is now &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1331r2.pdf"&gt;allowed&lt;/a&gt; in C++20. As a result, a &lt;code&gt;constexpr&lt;/code&gt; constructor doesn&amp;#8217;t necessarily have to initialize all the fields (but reading an uninitialized object is, of course, still forbidden):&lt;/p&gt; &lt;pre&gt;struct S { int i; int u; constexpr S() : i{1} { } }; constexpr S s; // error: refers to an incompletely initialized variable S s2; // OK constexpr int fn (int n) { int a; a = 5; return a + n; } &lt;/pre&gt; &lt;h2&gt;&lt;code&gt;constexpr dynamic_cast&lt;/code&gt;&lt;/h2&gt; &lt;p&gt;In &lt;code&gt;constexpr&lt;/code&gt; contexts, you can now evaluate a &lt;code&gt;dynamic_cast&lt;/code&gt; &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1327r1.html"&gt;at compile time&lt;/a&gt;. Virtual function calls in constant expressions were already permitted, so this proposal made it valid to use a &lt;code&gt;constexpr&lt;/code&gt; &lt;code&gt;dynamic_cast&lt;/code&gt;, like this:&lt;/p&gt; &lt;pre&gt;struct B { virtual void baz () {} }; struct D : B { }; constexpr D d; constexpr B *b = const_cast&amp;#60;D*&amp;#62;(&amp;#38;d); static_assert(dynamic_cast&amp;#60;D*&amp;#62;(b) == &amp;#38;d); &lt;/pre&gt; &lt;p&gt;Previously, a &lt;code&gt;constexpr&lt;/code&gt; &lt;code&gt;dynamic_cast&lt;/code&gt; would have required a runtime call to a function defined by the C++ runtime library. Similarly, a polymorphic &lt;code&gt;typeid&lt;/code&gt; is now also allowed in &lt;code&gt;constexpr&lt;/code&gt; contexts.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: C++20 modules are not supported in GCC 10; they are still a work in progress (here is &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1103r3.pdf"&gt;a related proposal&lt;/a&gt;). We hope to include them in GCC 11.&lt;/p&gt; &lt;h2&gt;Additional updates&lt;/h2&gt; &lt;p&gt;In non-C++20 news, the C++ compiler now detects modifying constant objects in &lt;code&gt;constexpr&lt;/code&gt; evaluation, which is undefined behavior:&lt;/p&gt; &lt;pre&gt;constexpr int fn () { const int i = 5; const_cast&amp;#60;int &amp;#38;&amp;#62;(i) = 10; // error: modifying a const object return i; } constexpr int i = fn (); &lt;/pre&gt; &lt;p&gt;GCC also handles the case when a constant object under construction is being modified and doesn&amp;#8217;t emit an error in that case.&lt;/p&gt; &lt;h3&gt;Narrowing conversions&lt;/h3&gt; &lt;p&gt;Narrowing conversions are invalid in certain contexts, such as list initialization:&lt;/p&gt; &lt;pre&gt;int i{1.2}; &lt;/pre&gt; &lt;p&gt;GCC 10 is able to detect narrowing in more contexts where it&amp;#8217;s invalid, for instance, case values in a &lt;code&gt;switch&lt;/code&gt; statement:&lt;/p&gt; &lt;pre&gt;void g(int i) { switch (i) case __INT_MAX__ + 1u:; } &lt;/pre&gt; &lt;h3&gt;The &lt;code&gt;noexcept&lt;/code&gt; specifier&lt;/h3&gt; &lt;p&gt;GCC 10 properly treats the &lt;code&gt;noexcept&lt;/code&gt; specifier as a &lt;em&gt;complete-class context&lt;/em&gt;. As with member function bodies, default arguments, and non-static data member initializers, you can declare names used in a member function&amp;#8217;s &lt;code&gt;noexcept&lt;/code&gt; specifier later in the class body. The following valid C++ code could not be compiled with GCC 9, but it compiles with GCC 10:&lt;/p&gt; &lt;pre&gt;struct S { void foo() noexcept(b); static constexpr auto b = true; }; &lt;/pre&gt; &lt;h3&gt;The &lt;code&gt;deprecated&lt;/code&gt; attribute&lt;/h3&gt; &lt;p&gt;You can now use the &lt;code&gt;deprecated&lt;/code&gt; attribute on namespaces:&lt;/p&gt; &lt;pre&gt;namespace v0 [[deprecated("oh no")]] { int fn (); } void g () { int x = v0::fn (); // warning: v0 is deprecated } &lt;/pre&gt; &lt;p&gt;GCC 9 compiles this code but ignores the attribute, whereas GCC 10 correctly warns about using entities from the deprecated namespace.&lt;/p&gt; &lt;h3&gt;Defect report resolutions&lt;/h3&gt; &lt;p&gt;We resolved several defect reports (DRs) in GCC 10. One example is &lt;a target="_blank" rel="nofollow" href="http://wg21.link/cwg1710"&gt;DR 1710&lt;/a&gt;, which says that when we are naming a type, the &lt;code&gt;template&lt;/code&gt; keyword is optional. Therefore, the following test compiles without errors with GCC 10:&lt;/p&gt; &lt;pre&gt;template&amp;#60;typename T&amp;#62; struct S { void fn(typename T::template B&amp;#60;int&amp;#62;::template C&amp;#60;int&amp;#62;); void fn2(typename T::B&amp;#60;int&amp;#62;::template C&amp;#60;int&amp;#62;); void fn3(typename T::template B&amp;#60;int&amp;#62;::C&amp;#60;int&amp;#62;); void fn4(typename T::B&amp;#60;int&amp;#62;::C&amp;#60;int&amp;#62;); }; &lt;/pre&gt; &lt;p&gt;Another interesting DR is &lt;a target="_blank" rel="nofollow" href="http://wg21.link/cwg1307"&gt;DR 1307&lt;/a&gt;, which clarified how overload resolution ought to behave when it comes to choosing a better candidate based on the size-of-array initializer list. Consider the following test:&lt;/p&gt; &lt;pre&gt;void f(int const(&amp;#38;)[2]); void f(int const(&amp;#38;)[3]) = delete; void g() { f({1, 2}); } &lt;/pre&gt; &lt;p&gt;GCC 9 rejects this test because it can&amp;#8217;t decide which candidate is better. Yet it seems obvious that the first candidate is best (never mind the &lt;code&gt;= delete&lt;/code&gt; part; deleted functions participate in overload resolution). GCC 10 chooses this option, which lets the code compile.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: You can find the overall defect resolution status on the &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/projects/cxx-dr-status.html"&gt;C++ Defect Report Support in GCC&lt;/a&gt; page.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In GCC 11, we plan to finish up the remaining C++20 features. For progress so far, see the &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/projects/cxx-status.html#cxx2a"&gt;C++2a Language Features&lt;/a&gt; table on the &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/projects/cxx-status.html"&gt;C++ Standards Support in GCC&lt;/a&gt; page. GCC 11 will also switch the default dialect to C++17 (it has already happened). Please do not hesitate to &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/bugs/"&gt;file bugs&lt;/a&gt; in the meantime, and help us make GCC even better!&lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;I&amp;#8217;d like to thank my coworkers at Red Hat who made the GNU C++ compiler so much better, notably Jason Merrill, Jakub Jelinek, Patrick Palka, and Jonathan Wakely.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#038;title=New%20C%2B%2B%20features%20in%20GCC%2010" data-a2a-url="https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/" data-a2a-title="New C++ features in GCC 10"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/"&gt;New C++ features in GCC 10&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JYKdxbvy15s" height="1" width="1" alt=""/&gt;</content><summary>The GNU Compiler Collection (GCC) 10.1 was released in May 2020. Like every other GCC release, this version brought many additions, improvements, bug fixes, and new features. Fedora 32 already ships GCC 10 as the system compiler, but it’s also possible to try GCC 10 on other platforms (see godbolt.org, for example). Red Hat Enterprise Linux (RHEL) users will get GCC 10 in the Red Hat Developer Too...</summary><dc:creator>Marek Polacek</dc:creator><dc:date>2020-09-24T07:00:32Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/</feedburner:origLink></entry><entry><title>Set up continuous integration for .NET Core with OpenShift Pipelines</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QReTQpTE1tg/" /><category term=".NET Core" scheme="searchisko:content:tags" /><category term=".net core ci" scheme="searchisko:content:tags" /><category term="C#" scheme="searchisko:content:tags" /><category term="ci/cd" scheme="searchisko:content:tags" /><category term="ci/cd pipeline" scheme="searchisko:content:tags" /><category term="CodeReady Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="tekton pipelines" scheme="searchisko:content:tags" /><author><name>Omair Majid</name></author><id>searchisko:content:id:jbossorg_blog-set_up_continuous_integration_for_net_core_with_openshift_pipelines</id><updated>2020-09-24T07:00:09Z</updated><published>2020-09-24T07:00:09Z</published><content type="html">&lt;p&gt;Have you ever wanted to set up &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration&lt;/a&gt; (CI) for &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET Core&lt;/a&gt; in a cloud-native way, but you didn&amp;#8217;t know where to start? This article provides an overview, examples, and suggestions for developers who want to get started setting up a functioning &lt;a href="https://developers.redhat.com/blog/2020/08/14/introduction-to-cloud-native-ci-cd-with-tekton-kubecon-europe-2020/"&gt;cloud-native CI&lt;/a&gt; system for .NET Core.&lt;/p&gt; &lt;p&gt;We will use the new &lt;a href="https://developers.redhat.com/blog/2020/04/30/creating-pipelines-with-openshift-4-4s-new-pipeline-builder-and-tekton-pipelines/"&gt;Red Hat OpenShift Pipelines&lt;/a&gt; feature to implement .NET Core CI. &lt;a href="https://developers.redhat.com/blog/2020/04/27/modern-web-applications-on-openshift-part-4-openshift-pipelines"&gt;OpenShift Pipelines&lt;/a&gt; are based on the open source &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/pipeline#-tekton-pipelines"&gt;Tekton&lt;/a&gt; project. OpenShift Pipelines provide a cloud-native way to define a pipeline to build, test, deploy, and roll out your applications in a continuous integration workflow.&lt;/p&gt; &lt;p&gt;In this article, you will learn how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Set up a simple .NET Core application.&lt;/li&gt; &lt;li&gt;Install OpenShift Pipelines on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Create a simple pipeline manually.&lt;/li&gt; &lt;li&gt;Create a Source-to-Image (S2I)-based pipeline.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;span id="more-747217"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You will need cluster-administrator access to an OpenShift instance to be able to access the example application and follow all of the steps described in this article. If you don&amp;#8217;t have access to an OpenShift instance, or if you don&amp;#8217;t have cluster-admin privileges, you can run an OpenShift instance locally on your machine using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt;. Running OpenShift locally should be as easy as &lt;code&gt;crc setup&lt;/code&gt; followed by &lt;code&gt;crc start&lt;/code&gt;. Also, be sure to &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/cli_reference/openshift_cli/getting-started-cli.html"&gt;install&lt;/a&gt; the &lt;code&gt;oc&lt;/code&gt; tool; we will use it throughout the examples.&lt;/p&gt; &lt;p&gt;When I wrote this article, I was using:&lt;/p&gt; &lt;pre&gt;$ crc version CodeReady Containers version: 1.12.0+6710aff OpenShift version: 4.4.8 (embedded in binary) &lt;/pre&gt; &lt;h2&gt;The example application&lt;/h2&gt; &lt;p&gt;To start, let&amp;#8217;s review the .NET Core project that we will use for the remainder of the article. It&amp;#8217;s a simple &amp;#8220;Hello, World&amp;#8221;-style web application that is built on top of ASP.NET Core.&lt;/p&gt; &lt;p&gt;The complete application is available from the Red Hat Developer &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex"&gt;s2i-dotnetcore-ex&lt;/a&gt; GitHub repository, in the branch &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/tree/dotnetcore-3.1-openshift-manual-pipeline"&gt;dotnetcore-3.1-openshift-manual-pipeline&lt;/a&gt;. You can use this repository and branch directly for the steps described in this article. If you want to make changes and test how they affect your pipeline, you can fork the repository and use the fork.&lt;/p&gt; &lt;p&gt;Everything is included in the source repository, including everything we need for continuous integration. There are at least a couple of advantages to this approach: Our complete setup is tracked and reproducible, and we can easily code-review changes we might do later on, including any changes to our CI pipeline.&lt;/p&gt; &lt;h3&gt;Project directories&lt;/h3&gt; &lt;p&gt;Before we continue, let&amp;#8217;s review the &lt;code&gt;dotnetcore-3.1-openshift-manual-pipeline&lt;/code&gt; branch of the repository.&lt;/p&gt; &lt;p&gt;This project contains two main directories: &lt;code&gt;app&lt;/code&gt; and &lt;code&gt;app.tests&lt;/code&gt;. The &lt;code&gt;app&lt;/code&gt; directory contains the application code. This app was essentially created by &lt;code&gt;dotnet new mvc&lt;/code&gt;. The &lt;code&gt;app.tests&lt;/code&gt; directory contains unit tests that we want to run when we build the app. This was created by &lt;code&gt;dotnet new xunit&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Another directory called &lt;code&gt;ci&lt;/code&gt; contains the configuration we will use for our CI setup. I will describe the files in that directory in detail as we go through the examples.&lt;/p&gt; &lt;p&gt;Now, let&amp;#8217;s get to the fun part.&lt;/p&gt; &lt;h2&gt;Install OpenShift Pipelines&lt;/h2&gt; &lt;p&gt;Log into the OpenShift cluster as an administrator and install OpenShift Pipelines. You can use the &lt;b&gt;OperatorHub&lt;/b&gt; in your OpenShift console, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_747247" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01.png"&gt;&lt;img aria-describedby="caption-attachment-747247" class="wp-image-747247" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01.png" alt="The OpenShift Pipelines Operator installation screen." width="640" height="472" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01.png 913w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01-300x221.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01-768x567.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747247" class="wp-caption-text"&gt;Figure 1: Use the OpenShift OperatorHub to install OpenShift Pipelines.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;See the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/pipelines/installing-pipelines.html"&gt;OpenShift documentation&lt;/a&gt; for a complete guide to installing the OpenShift Pipelines Operator.&lt;/p&gt; &lt;h2&gt;Develop a pipeline manually&lt;/h2&gt; &lt;p&gt;Now that we have our .NET Core application code and an OpenShift instance ready to use, we can create an Openshift Pipelines-based system for building this code. For our first example, we&amp;#8217;ll create a pipeline manually.&lt;/p&gt; &lt;p&gt;Before we create the pipeline, let&amp;#8217;s start with a few things you need to know about OpenShift Pipelines.&lt;/p&gt; &lt;h3&gt;About OpenShift Pipelines&lt;/h3&gt; &lt;p&gt;OpenShift Pipelines offer a flexible way to build and deploy applications. OpenShift Pipelines are not opinionated: You can do pretty much anything you want as part of a pipeline. To be this flexible, OpenShift Pipelines lets us define several different types of objects. These objects represent various parts of the pipeline, such as inputs, output, stages, and even connections between these elements. We can assemble these objects however we need to, to get the build-test-deploy pipeline that we want.&lt;/p&gt; &lt;p&gt;Here are a few types of objects that we will use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;OpenShift Pipelines &lt;code&gt;Task&lt;/code&gt;&lt;/strong&gt; is a collection of &lt;code&gt;step&lt;/code&gt;s that we want to run in order. There are many predefined &lt;code&gt;Task&lt;/code&gt;s, and we can make more as we need them. &lt;code&gt;Task&lt;/code&gt;s can have inputs and outputs. They can also have parameters that control aspects of the &lt;code&gt;Task&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/strong&gt; is a collection of &lt;code&gt;Task&lt;/code&gt;s. It lets us group &lt;code&gt;Task&lt;/code&gt;s and run them in a particular order, and it lets us connect the output of one &lt;code&gt;Task&lt;/code&gt; to the input of another.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;PipelineResource&lt;/code&gt;&lt;/strong&gt; indicates a &lt;code&gt;Task&lt;/code&gt; or&amp;#8217;s input or output. There are several types, including &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;image&lt;/code&gt;. The &lt;code&gt;git&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; indicates a Git repository, which is often used as in the input to a &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;Task&lt;/code&gt;. An &lt;code&gt;image&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; indicates a container image that we can use via the built-in OpenShift container registry.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These objects describe what a &lt;code&gt;Pipeline&lt;/code&gt; is. We need at least two more things to run a pipeline. Creating and running a pipeline are two distinct operations in OpenShift. You can create a pipeline and never run it. Or, you can re-run the same pipeline multiple times.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;TaskRun&lt;/code&gt; &lt;/strong&gt;describes how to run a &lt;code&gt;Task&lt;/code&gt;. It connects the inputs and outputs of a &lt;code&gt;Task&lt;/code&gt; with &lt;code&gt;PipelineResource&lt;/code&gt;s.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;PipelineRun&lt;/code&gt;&lt;/strong&gt; describes how to run a &lt;code&gt;Pipeline&lt;/code&gt;. It connects the pipeline&amp;#8217;s inputs and outputs with &lt;code&gt;PipelineResource&lt;/code&gt;s. A &lt;code&gt;PipelineRun&lt;/code&gt; implicitly contains one or more &lt;code&gt;TaskRun&lt;/code&gt;s, so we can skip creating &lt;code&gt;TaskRun&lt;/code&gt;s manually.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now we&amp;#8217;re ready to put all of these elements into action.&lt;/p&gt; &lt;h3&gt;Step 1: Create a new pipeline project&lt;/h3&gt; &lt;p&gt;First, we&amp;#8217;ll create a new project to work in OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc new-project dotnet-pipeline-app&lt;/pre&gt; &lt;p&gt;To make a functional pipeline, we will need to define a few objects that we can add to a single file, &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/simple/simple-pipeline.yaml"&gt;simple-pipeline.yaml&lt;/a&gt;. This file is located in the &lt;code&gt;ci/simple/simple-pipeline.yaml&lt;/code&gt; file in the repository that we are using. To separate these objects, we will use a dash (&lt;code&gt;---&lt;/code&gt;) by itself on a single line.&lt;/p&gt; &lt;h3&gt;Step 2: Define a PipelineResource&lt;/h3&gt; &lt;p&gt;Next, we&amp;#8217;ll define a &lt;code&gt;PipelineResource&lt;/code&gt;. The &lt;code&gt;PipelineResource&lt;/code&gt; will provide the source Git repository to the rest of the pipeline:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: simple-dotnet-project-source spec: type: git params: - name: revision value: dotnetcore-3.1-openshift-manual-pipeline - name: url value: https://github.com/redhat-developer/s2i-dotnetcore-ex &lt;/pre&gt; &lt;p&gt;An object of &lt;code&gt;kind: PipelineResource&lt;/code&gt; has a name (&lt;code&gt;metadata.name&lt;/code&gt;) and a type of resource (&lt;code&gt;spec.type&lt;/code&gt;). Here, we use the type &lt;code&gt;git&lt;/code&gt; to indicate that this &lt;code&gt;PipelineResource&lt;/code&gt; represents a Git repository. The parameters to this &lt;code&gt;PipelineResource&lt;/code&gt; specify the Git repository that we want to use.&lt;/p&gt; &lt;p&gt;If you are using your own fork, please adjust the URL and branch name.&lt;/p&gt; &lt;h3&gt;Step 3: Define the pipeline&lt;/h3&gt; &lt;p&gt;Next, we define a pipeline. The &lt;code&gt;Pipeline&lt;/code&gt; instance will coordinate all of the tasks that we want to run:&lt;/p&gt; &lt;pre&gt;apapiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: simple-dotnet-pipeline spec: resources: - name: source-repository type: git tasks: - name: simple-build-and-test taskRef: name: simple-publish resources: inputs: - name: source resource: source-repository &lt;/pre&gt; &lt;p&gt;This &lt;code&gt;Pipeline&lt;/code&gt; uses a resource (a &lt;code&gt;PipelineResource&lt;/code&gt;) with the name of &lt;code&gt;source-repository&lt;/code&gt;, which should be a &lt;code&gt;git&lt;/code&gt; repository. It uses just one &lt;code&gt;Task&lt;/code&gt; (via the &lt;code&gt;taskRef&lt;/code&gt; with a &lt;code&gt;name&lt;/code&gt; of &lt;code&gt;simple-publish&lt;/code&gt;) that will build our source code. We will connect the input of the pipeline (&lt;code&gt;source-repository&lt;/code&gt; resource) to the input of the task (&lt;code&gt;source&lt;/code&gt; input resource).&lt;/p&gt; &lt;h3&gt;Step 4: Define and build a task&lt;/h3&gt; &lt;p&gt;Next, we define a simple &lt;code&gt;Task&lt;/code&gt; that takes a &lt;code&gt;git&lt;/code&gt; source repository. Then, we build the task:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: simple-publish spec: resources: inputs: - name: source type: git steps: - name: simple-dotnet-publish image: registry.access.redhat.com/ubi8/dotnet-31 # .NET Core SDK securityContext: runAsUser: 0  # UBI 8 images generally run as non-root script: | #!/usr/bin/bash dotnet --info cd source dotnet publish -c Release -r linux-x64 --self-contained false "app/app.csproj" &lt;/pre&gt; &lt;p&gt;This &lt;code&gt;Task&lt;/code&gt; takes a &lt;code&gt;PipelineResource&lt;/code&gt; of type &lt;code&gt;git&lt;/code&gt; that represents the application source code. It runs the listed &lt;code&gt;steps&lt;/code&gt; to build the source code using the &lt;code&gt;ubi8/dotnet-31&lt;/code&gt; container image. This container image is published by Red Hat and is optimized for building .NET Core applications in &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) and OpenShift. We also specify, via &lt;code&gt;script&lt;/code&gt;, the exact steps to build our .NET Core application, which is essentially just a &lt;code&gt;dotnet publish&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Step 5: Describe and apply the pipeline&lt;/h3&gt; &lt;p&gt;We have everything that we need to describe our pipeline. Our final &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/simple/simple-pipeline.yaml"&gt;ci/simple/simple-pipeline.yaml&lt;/a&gt; file should look like this:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: simple-dotnet-project-source spec: type: git params: - name: revision value: dotnetcore-3.1-openshift-manual-pipeline - name: url value: https://github.com/redhat-developer/s2i-dotnetcore-ex --- apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: simple-dotnet-pipeline spec: resources: - name: source-repository type: git tasks: - name: simple-build-and-test taskRef: name: simple-publish resources: inputs: - name: source resource: source-repository --- apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: simple-publish spec: resources: inputs: - name: source type: git steps: - name: simple-dotnet-publish image: registry.access.redhat.com/ubi8/dotnet-31 # .NET Core SDK securityContext: runAsUser: 0  # UBI 8 images generally run as non-root script: | #!/usr/bin/bash dotnet --info cd source dotnet publish -c Release -r linux-x64 --self-contained false "app/app.csproj" &lt;/pre&gt; &lt;p&gt;We can now apply it to our OpenShift instance:&lt;/p&gt; &lt;pre&gt;$ oc apply -f ci/simple/simple-pipeline.yaml&lt;/pre&gt; &lt;p&gt;Applying the pipeline to our OpenShift instance makes OpenShift modify its currently-active configuration to match what we have specified in the YAML file. That includes creating any objects (such as &lt;code&gt;Pipeline&lt;/code&gt;s, &lt;code&gt;PipelineResource&lt;/code&gt;s, and &lt;code&gt;Task&lt;/code&gt;s) that did not previously exist. If objects of the same name and types already exist, they will be modified to match what we have specified in our YAML file.&lt;/p&gt; &lt;p&gt;If you look in &lt;b&gt;Pipelines&lt;/b&gt; section of the OpenShift Developer console (&lt;code&gt;crc console&lt;/code&gt; if you are using CodeReady Containers), you should see that this pipeline is now available, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_747347" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02.png"&gt;&lt;img aria-describedby="caption-attachment-747347" class="wp-image-747347 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-1024x401.png" alt="The OpenShift dashboard showing the newly created pipeline." width="640" height="251" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-1024x401.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-300x117.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-768x301.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02.png 1344w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747347" class="wp-caption-text"&gt;Figure 2: The new pipeline in the OpenShift developer console.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Did you notice that the pipeline is not running? We&amp;#8217;ll deal with that next.&lt;/p&gt; &lt;h3&gt;Step 6: Run the pipeline&lt;/h3&gt; &lt;p&gt;To run the pipeline, we will create a &lt;code&gt;PipelineRun&lt;/code&gt; object that associates the &lt;code&gt;PipelineResource&lt;/code&gt; with the &lt;code&gt;Pipeline&lt;/code&gt; itself, and actually runs everything. The &lt;code&gt;PipelineRun&lt;/code&gt; object for this example is &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/simple/run-simple-pipeline.yaml"&gt;ci/simple/run-simple-pipeline.yaml&lt;/a&gt;, as shown here:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: run-simple-dotnet-pipeline- spec: pipelineRef: name: simple-dotnet-pipeline resources: - name: source-repository resourceRef: name: simple-dotnet-project-source &lt;/pre&gt; &lt;p&gt;This YAML file defines an object of &lt;code&gt;kind: PipelineRun&lt;/code&gt;, which uses the &lt;code&gt;simple-dotnet-pipeline&lt;/code&gt; (indicated with the &lt;code&gt;pipelineRef&lt;/code&gt;). It associates the &lt;code&gt;simple-dotnet-project-source&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; that we defined earlier with the input resource that the &lt;code&gt;simple-dotnet-pipeline&lt;/code&gt; expects.&lt;/p&gt; &lt;p&gt;Next, we will use &lt;code&gt;oc create&lt;/code&gt; (instead of &lt;code&gt;oc apply&lt;/code&gt;) to create and run the pipeline:&lt;/p&gt; &lt;pre&gt;$ oc create -f ci/simple/run-simple-pipeline.yaml&lt;/pre&gt; &lt;p&gt;We need to use &lt;code&gt;oc create&lt;/code&gt; here because each &lt;code&gt;PipelineRun&lt;/code&gt; represents an actual invocation of the pipeline. We don&amp;#8217;t want to &lt;code&gt;apply&lt;/code&gt; an update to a previous run of a &lt;code&gt;Pipeline&lt;/code&gt;. We want to run the pipeline (again).&lt;/p&gt; &lt;p&gt;This is also why we used &lt;code&gt;generateName&lt;/code&gt; instead of &lt;code&gt;name&lt;/code&gt; when defining the pipeline YAML. Each OpenShift object has a unique name, including a &lt;code&gt;PipelineRun&lt;/code&gt;. If we gave our pipeline a concrete name, we wouldn&amp;#8217;t be able to &lt;code&gt;oc create&lt;/code&gt; it again. To work around this, we tell OpenShift to generate a new name, prefixing it with our provided value. This allows us to run &lt;code&gt;oc create -f ci/simple/simple-pipeline.yaml&lt;/code&gt; multiple times. Each time we enter this command, it will run the pipeline in OpenShift.&lt;/p&gt; &lt;p&gt;You should now see the pipeline running in the OpenShift console, as shown in Figure 3. Wait a little bit, and it will complete.&lt;/p&gt; &lt;div id="attachment_747357" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03.png"&gt;&lt;img aria-describedby="caption-attachment-747357" class="wp-image-747357 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-1024x520.png" alt="A terminal showing the pipeline running." width="640" height="325" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-1024x520.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-768x390.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03.png 1081w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747357" class="wp-caption-text"&gt;Figure 3: The manually created pipeline running in the OpenShift console.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Develop an S2I-based pipeline&lt;/h2&gt; &lt;p&gt;In the previous section, we created a pipeline manually. For that, we had to write out all the steps to publish our application. We had to list out the exact steps to build our code, as well as the container image we wanted to use to build it. We also would have had to list out any other steps, such as running tests.&lt;/p&gt; &lt;p&gt;We can make our lives easier by re-using predefined tasks. The predefined &lt;code&gt;s2i-dotnet-3&lt;/code&gt; &lt;code&gt;Task&lt;/code&gt; already knows how to build a .NET Core application using the &lt;code&gt;s2i-dotnetcore&lt;/code&gt; images. In this section, you&amp;#8217;ll learn how to build an S2I-based pipeline.&lt;/p&gt; &lt;h3&gt;Step 1: Install the .NET Core &lt;code&gt;s2i&lt;/code&gt; tasks&lt;/h3&gt; &lt;p&gt;Let&amp;#8217;s start by installing the &lt;code&gt;s2i&lt;/code&gt; tasks for .NET Core 3.x:&lt;/p&gt; &lt;pre&gt;$ oc apply -f https://raw.githubusercontent.com/openshift/pipelines-catalog/master/task/s2i-dotnet-3-pr/0.1/s2i-dotnet-3-pr.yaml &lt;/pre&gt; &lt;h3&gt;Step 2: Add an &lt;code&gt;.s2i/environment&lt;/code&gt; file to the repository&lt;/h3&gt; &lt;p&gt;We will also need to use a few new files in our code repository to tell the S2I build system what .NET Core projects to use for testing and deployment. We can do that by adding a &lt;code&gt;.s2i/environment&lt;/code&gt; file to the repository with the following two lines:&lt;/p&gt; &lt;pre&gt;DOTNET_STARTUP_PROJECT=app/app.csproj DOTNET_TEST_PROJECTS=app.tests/app.tests.csproj &lt;/pre&gt; &lt;p&gt;&lt;code&gt;DOTNET_STARTUP_PROJECT&lt;/code&gt; specifies what project (or application) we want to run in the final built container. &lt;code&gt;DOTNET_TEST_PROJECTS&lt;/code&gt; specifies which projects (if any) we want to run to test our code before we build the application container.&lt;/p&gt; &lt;h3&gt;Step 3: Define the pipeline&lt;/h3&gt; &lt;p&gt;Now, let&amp;#8217;s define a &lt;code&gt;Pipeline&lt;/code&gt;, &lt;code&gt;PipelineResource&lt;/code&gt;s, and &lt;code&gt;Task&lt;/code&gt;s in a single pipeline file. This time, we will use the &lt;code&gt;s2i-dotnet-3-pr&lt;/code&gt; task. We will also include another &lt;code&gt;PipelineResource&lt;/code&gt; with &lt;code&gt;type: image&lt;/code&gt; that represents the output container image. This pipeline will build the source code and then push the just-built application image into OpenShift&amp;#8217;s built-in container registry. Our &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/s2i/s2i-pipeline.yaml"&gt;ci/s2i/s2i-pipeline.yaml&lt;/a&gt; should look like this:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: s2i-dotnet-project-source spec: type: git params: - name: revision value: dotnetcore-3.1-openshift-manual-pipeline - name: url value: https://github.com/redhat-developer/s2i-dotnetcore-ex --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: s2i-dotnet-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/dotnet-pipeline-app/app:latest --- apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: s2i-dotnet-pipeline spec: resources: - name: source-repository type: git - name: image type: image tasks: - name: s2i-build-source taskRef: name: s2i-dotnet-3-pr params: - name: TLSVERIFY value: "false" resources: inputs: - name: source resource: source-repository outputs: - name: image resource: image &lt;/pre&gt; &lt;p&gt;Note how we provided the &lt;code&gt;s2i-dotnet-3-pr&lt;/code&gt; task with the &lt;code&gt;source-repository&lt;/code&gt; resource and the &lt;code&gt;image&lt;/code&gt; resource. This task already knows how to build a .NET Core application from source code (based on the contents of the &lt;code&gt;.s2i/environment&lt;/code&gt; file) and create a container image that includes the just-built application.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please keep in mind that you can only push the container image to the same namespace as your OpenShift project, which in this case is &lt;code&gt;dotnet-pipeline-app&lt;/code&gt;. If we tried using a container name that placed the container image somewhere other than &lt;code&gt;/dotnet-pipeline-app/&lt;/code&gt;, we would encounter permission issues.&lt;/p&gt; &lt;h3&gt;Step 4: Apply the pipeline in OpenShift&lt;/h3&gt; &lt;p&gt;Now let&amp;#8217;s set up this pipeline in OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc apply -f ci/s2i/s2i-pipeline.yaml &lt;/pre&gt; &lt;p&gt;The new &lt;code&gt;s2i-dotnet-pipeline&lt;/code&gt; should now show up in the &lt;b&gt;Pipelines&lt;/b&gt; section of the OpenShift developer console, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_747387" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04.png"&gt;&lt;img aria-describedby="caption-attachment-747387" class="wp-image-747387 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-1024x445.png" alt="The OpenShift dashboard showing the S2I pipeline running." width="640" height="278" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-1024x445.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-300x130.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-768x333.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04.png 1085w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747387" class="wp-caption-text"&gt;Figure 4: The new pipeline in the OpenShift developer console.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Step 5: Run the pipeline&lt;/h3&gt; &lt;p&gt;To run this pipeline, we will create another &lt;code&gt;PipelineRun&lt;/code&gt;, just like we did in the previous example. Here&amp;#8217;s our ci/s2i/run-s2i-pipeline. &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/s2i/run-s2i-pipeline.yaml"&gt;yaml&lt;/a&gt; file:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: run-s2i-dotnet-pipeline- spec: serviceAccountName: pipeline pipelineRef: name: s2i-dotnet-pipeline resources: - name: source-repository resourceRef: name: s2i-dotnet-project-source - name: image resourceRef: name: s2i-dotnet-image &lt;/pre&gt; &lt;p&gt;Run it with:&lt;/p&gt; &lt;pre&gt;$ oc create -f ci/s2i/run-s2i-pipeline.yaml &lt;/pre&gt; &lt;p&gt;When you look at the OpenShift developer console, you can see the pipeline running (or having completed). The pipeline log should look something like what&amp;#8217;s shown in Figure 5:&lt;/p&gt; &lt;div id="attachment_747407" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05.png"&gt;&lt;img aria-describedby="caption-attachment-747407" class="wp-image-747407 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-1024x771.png" alt="The OpenShift terminal showing the S2I pipeline running." width="640" height="482" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-1024x771.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-300x226.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-768x578.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05.png 1088w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747407" class="wp-caption-text"&gt;Figure 5: The new pipeline running in the OpenShift developer console.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This time, we can see that the pipeline started up, ran all of our tests, published our application, created a container image with it, and pushed the just-built container into the built-in container registry in OpenShift.&lt;/p&gt; &lt;p&gt;Nice, isn&amp;#8217;t it?&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you followed along with this article, then you should now know how to create a code-first implementation of cloud-ready CI pipelines to build, test, and publish container images for your .NET Core applications. Now that you are familiar with this procedure, why not try it out for your applications?&lt;/p&gt; &lt;p&gt;If you are looking for ideas to further what you&amp;#8217;ve learned, here are some suggestions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We triggered all of our pipeline runs manually, but that&amp;#8217;s obviously not what you want for a real continuous integration setup. Consider extending the pipeline to run &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/pipelines/creating-applications-with-cicd-pipelines.html#about-triggers_creating-applications-with-cicd-pipelines"&gt;on triggers and webhooks&lt;/a&gt; from your source repository.&lt;/li&gt; &lt;li&gt;We limited our CI pipeline to stop after building and publishing container images. Try creating a pipeline that goes all the way to deploying your project in OpenShift.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Additional resources&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Learn more about Red Hat &lt;a target="_blank" rel="nofollow" href="https://code-ready.github.io/crc"&gt;CodeReady Containers&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get complete instructions for &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/pipelines/creating-applications-with-cicd-pipelines.html"&gt;creating applications with OpenShift Pipelines&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Check out the &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/catalog"&gt;Tekton Task/Pipelines catalog&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Also, see the &lt;a target="_blank" rel="nofollow" href="https://github.com/openshift/pipelines-catalog"&gt;OpenShift Pipelines catalog&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get the &lt;a target="_blank" rel="nofollow" href="https://github.com/openshift/pipelines-catalog/tree/master/task"&gt;s2i dotnet task&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get the &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore"&gt;S2I container images&lt;/a&gt; for .NET Core.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#038;title=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" data-a2a-url="https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/" data-a2a-title="Set up continuous integration for .NET Core with OpenShift Pipelines"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/"&gt;Set up continuous integration for .NET Core with OpenShift Pipelines&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QReTQpTE1tg" height="1" width="1" alt=""/&gt;</content><summary>Have you ever wanted to set up continuous integration (CI) for .NET Core in a cloud-native way, but you didn’t know where to start? This article provides an overview, examples, and suggestions for developers who want to get started setting up a functioning cloud-native CI system for .NET Core. We will use the new Red Hat OpenShift Pipelines feature to implement .NET Core CI. OpenShift Pipelines ar...</summary><dc:creator>Omair Majid</dc:creator><dc:date>2020-09-24T07:00:09Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/</feedburner:origLink></entry><entry><title>Payments Architecture - Immediate Payments Example</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/IjfMUYbRB_w/payments-architecture-immediate-payments-example.html" /><category term="Architecture Blueprints" scheme="searchisko:content:tags" /><category term="Automate" scheme="searchisko:content:tags" /><category term="Decision Manager" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="FUSE" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="JBossAMQ" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Process Automation Manager" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-payments_architecture_immediate_payments_example</id><updated>2020-09-24T09:10:26Z</updated><published>2020-09-24T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; margin-right: 1em;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-NhImz2b--gU/X1jsrXrG_jI/AAAAAAAAxeI/2I4wj4AD4YUcuxWk1-464UVs5OiejZFwQCNcBGAsYHQ/s1600/christiann-koepke-0jPuWm8_9wY-unsplash.jpg" style="clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;"&gt;&lt;img alt="payments architecture" border="0" data-original-height="1067" data-original-width="1600" height="213" src="https://1.bp.blogspot.com/-NhImz2b--gU/X1jsrXrG_jI/AAAAAAAAxeI/2I4wj4AD4YUcuxWk1-464UVs5OiejZFwQCNcBGAsYHQ/s320/christiann-koepke-0jPuWm8_9wY-unsplash.jpg" title="" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="font-size: 12.8px; text-align: center;"&gt;Part 3 - Immediate payments&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Cloud technology is changing the way payment services are architectured. In this series we will be presenting insight from our customers on adopting open source and cloud technology to modernize their payment service.&lt;br /&gt;&lt;br /&gt;So far we've presented research-based architectural blueprints of&amp;nbsp;&lt;a href="http://www.schabell.org/2018/11/integration-key-to-customer-experience-introduction.html" target="_blank"&gt;omnichannel customer experience&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.schabell.org/2020/01/integrating-saas-applications-an-introduction.html" target="_blank"&gt;integrating with SaaS applications&lt;/a&gt;, and&amp;nbsp;&lt;a href="https://www.schabell.org/2020/05/cloud-native-development-a-blueprint.html" target="_blank"&gt;cloud-native development solutions&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;In &lt;a href="https://www.schabell.org/2020/09/financial-payments-architecture-common-elements.html" target="_blank"&gt;the previous article&lt;/a&gt; in this series we explored the common architectural elements found in a payments logical architecture.&lt;br /&gt;&lt;br /&gt;In this article we'll walk through an immediate payments physical architecture,&amp;nbsp; laying out what a successful payments solution looks like in practice.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;&lt;h3 style="text-align: left;"&gt;Blueprints&lt;/h3&gt;&lt;div&gt;As a reminder, the architectural details covered here are base on real customer integration solutions using open source technologies.&lt;br /&gt;&lt;br /&gt;The example scenario presented here is a generic common blueprint that was uncovered researching customer solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details.&lt;br /&gt;&lt;br /&gt;This section covers the visual representations as presented. There are many ways to represent each element in this architectural blueprint, but we've chosen icons, text and colors that I hope are going to make it all easy to absorb. Feel free to post comments at the bottom of this post, or &lt;a href="https://www.schabell.org/p/contact.html" target="_blank"&gt;contact us&lt;/a&gt; directly with your feedback.&lt;br /&gt;&lt;br /&gt;Now let's take a look at the details in this blueprint and outline the example.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h3 style="text-align: left;"&gt;Immediate payments&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;The example blueprint shown on the right entitled &lt;i&gt;Immediate payments network example&lt;/i&gt;&amp;nbsp;outlines how an immediate payments solution is applied to a physical architecture. Note that this diagram is focusing on the highest level of the immediate payments solution and the element groupings that apply to this process.&lt;br /&gt;&lt;a href="https://1.bp.blogspot.com/-HYFXb2JUxzE/X2CtZHQlN5I/AAAAAAAAxgk/5gBIv7jWsNcPccDsOgeVkT0t9ec98zY1ACNcBGAsYHQ/s1600/payments-immediate-payments-network-sd.png" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="payments architecture" border="0" data-original-height="900" data-original-width="1600" height="180" src="https://1.bp.blogspot.com/-HYFXb2JUxzE/X2CtZHQlN5I/AAAAAAAAxgk/5gBIv7jWsNcPccDsOgeVkT0t9ec98zY1ACNcBGAsYHQ/s320/payments-immediate-payments-network-sd.png" title="" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;In this example, starting from the top left corner, a user sends an event or message to execute a payment as an entry point. The users can be mobile, web, or any external device / application that acts as the entry point with the organizations payments architecture.&lt;br /&gt;&lt;br /&gt;This request to execute payments connects to your services through the &lt;i&gt;payments API&lt;/i&gt;. This is the bridge to the internal central p&lt;i&gt;ayments event streams&lt;/i&gt;, where streams are managed to determine what selection or sub-selection of actions need to be taken. For practical purposes, we'll proceed through this architecture as if all selections are necessary to ensure coverage of all elements and services.&lt;br /&gt;&lt;br /&gt;The first action taken is validation of the incoming payments request, using the &lt;i&gt;validation microservices &lt;/i&gt;providing integration to all needed systems in an organization to validate funds, customers (users), and more. Once validation is completed a message is sent back to the &lt;i&gt;payments event streams&lt;/i&gt;&amp;nbsp;for further processing.&lt;br /&gt;&lt;br /&gt;The next two phases of the payments architecture are not always necessary as they depend on validation results. If there are any issues or flags raised on a payment request then the next step would be to trigger the use of the &lt;i&gt;anti-money laundering (AML) microservices &lt;/i&gt;or &lt;i&gt;fraud detection microservices. &lt;/i&gt;Each of these collections determine if action needs to be taken, if payment requests need to be blocked, if the user needs to be reported, and possibly blocking funds. Both of these service areas are leveraging &lt;i&gt;data caching&lt;/i&gt;&amp;nbsp;to ensure current data is available to the decision management tooling used to back these processing phases. Any results are then pushed back to the &lt;i&gt;payments event streams &lt;/i&gt;for further processing.&lt;br /&gt;&lt;br /&gt;Following the process elements to the right we'll arrive at the &lt;i&gt;clearing microservices&lt;/i&gt;&amp;nbsp;where processing for actual payment funds planning where accounts are debited before routing the funds to the requested parties. These results are sent back to the &lt;i&gt;payments event streams &lt;/i&gt;for further processing.&lt;br /&gt;&lt;br /&gt;Finally, &lt;i&gt;routing microservices &lt;/i&gt;are accessed to ensure the funds from the processed payments are distributed to the indicated parties through the available &lt;i&gt;payments network.&lt;/i&gt;&amp;nbsp;Note that the payments network is shown as an external secure cloud element, intended to indicate only that it's an external network and dependent on the region the solution is being deployed in for specifics as to the connection and data protocols used.&lt;br /&gt;&lt;a href="https://1.bp.blogspot.com/-qrlhtf3vujE/X2Ctmh0pIyI/AAAAAAAAxgo/jomUYBg9cak0uOrB7rxz1JjowWWVGwT4ACNcBGAsYHQ/s1600/payments-immediate-payments-data-sd.png" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img border="0" data-original-height="900" data-original-width="1600" height="180" src="https://1.bp.blogspot.com/-qrlhtf3vujE/X2Ctmh0pIyI/AAAAAAAAxgo/jomUYBg9cak0uOrB7rxz1JjowWWVGwT4ACNcBGAsYHQ/s320/payments-immediate-payments-data-sd.png" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;The second figure on the right here is labeled as an&amp;nbsp;&lt;i&gt;immediate payments data example &lt;/i&gt;and is meant to provide more insights into the movement of event streams and flow of data through the above described process of executing on a payments request. Exploring the details of this figure is left to the reader.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Project examples&lt;/h3&gt;&lt;div&gt;Sharing the process results for our payments blueprint is what this series is about, but there are project artifacts and diagrams that can also be shared with you the reader. We've pulled together an&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/portfolio-architecture-examples" target="_blank"&gt;examples repository&lt;/a&gt;&amp;nbsp;for all our architecture blueprint diagrams.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div style="text-align: right;"&gt;&lt;/div&gt;The&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/portfolio-architecture-examples" target="_blank"&gt;Portfolio Architecture Examples&lt;/a&gt;&amp;nbsp;repository makes it possible to collect and share individual images from each diagram element as well as the entire project as a whole.&lt;/div&gt;&lt;div&gt;&lt;table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: right; margin-left: 1em; text-align: right;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-4t4sRfvBdlA/X2CrzVQ9sFI/AAAAAAAAxgY/vZ61Z75fKhk3GFBC3ZZlOyGpIJWtBgDngCNcBGAsYHQ/s1600/Screenshot%2B2020-09-15%2Bat%2B13.55.42.png" style="clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;"&gt;&lt;img alt="payments architecture" border="1" data-original-height="232" data-original-width="530" height="139" src="https://1.bp.blogspot.com/-4t4sRfvBdlA/X2CrzVQ9sFI/AAAAAAAAxgY/vZ61Z75fKhk3GFBC3ZZlOyGpIJWtBgDngCNcBGAsYHQ/s320/Screenshot%2B2020-09-15%2Bat%2B13.55.42.png" title="" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Figure 1 - Physical diagrams in example repository.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For example, if you scroll down to the file listings on the main page, you can locate all the example physical diagrams as shown in figure 1.&lt;br /&gt;&lt;div style="text-align: right;"&gt;&lt;/div&gt;&lt;br /&gt;This is the collection associated with payments:&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;in this case there are multiple images you can click to view&lt;/li&gt;&lt;li&gt;a project file you can download to your local machine using the&amp;nbsp;&lt;i&gt;Download Diagram&lt;/i&gt;&amp;nbsp;link&lt;/li&gt;&lt;li&gt;a&amp;nbsp;&lt;i&gt;Load Diagram&lt;/i&gt;&amp;nbsp;link that you can &lt;a href="https://redhatdemocentral.gitlab.io/portfolio-architecture-tooling/index.html?#/portfolio-architecture-examples/projects/schematic-diagrams-payments.drawio" target="_blank"&gt;click to automatically open the project diagrams&lt;/a&gt; in the diagram tooling used in this blueprint (use private or incognito browser mode to avoid caching issues and a smoother tooling experience)&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;Give it a try and feel free to explore the collection of logical, schematic, detailed, solution, and community diagrams. This should allow you to get started much quicker than from scratch if you can kick-start a project with existing diagrams.&lt;br /&gt;&lt;br /&gt;Should you desire to start designing your own diagrams, please contribute the project file (ending in .drawio) by raising an issue with the file attached. We'd love to continue collecting these projects for others to use.&lt;br /&gt;&lt;br /&gt;Finally, there is a free online&amp;nbsp;&lt;a href="https://redhatdemocentral.gitlab.io/portfolio-architecture-workshops" target="_blank"&gt;beginners guide workshop&lt;/a&gt;&amp;nbsp;available focused on using the diagram tooling, please explore to learn tips and tricks from the experts.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;What's next&lt;/h3&gt;&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;An overview of the series on the payments portfolio architecture blueprint can be found here:&lt;br /&gt;&lt;ol style="text-align: left;"&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/financial-payments-architecture-an-introduction.html" target="_blank"&gt;An introduction&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/payments-architecture-common-elements.html" target="_blank"&gt;Common architecture elements&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/payments-architecture-immediate-payments-example.html" target="_blank"&gt;Immediate payments example&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Anti-money laundering example&lt;/li&gt;&lt;li&gt;Fraud detection example&lt;/li&gt;&lt;li&gt;Financial calculations example&lt;/li&gt;&lt;/ol&gt;&lt;ol style="text-align: left;"&gt;&lt;/ol&gt;Catch up on any articles you missed by following one of the links above.&lt;br /&gt;&lt;br /&gt;Next in this series, taking a look at the&amp;nbsp;generic&amp;nbsp;&lt;i&gt;anti-money laundering example&lt;/i&gt;&amp;nbsp;in a cloud-native architecture focused on payment processing.&lt;br /&gt;&lt;br /&gt;(Article co-authored by&amp;nbsp;&lt;a href="https://www.linkedin.com/in/ramonv/?originalSubdomain=uk" target="_blank"&gt;Ramon Villarreal&lt;/a&gt;)&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/7r9lufX4aUk" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/IjfMUYbRB_w" height="1" width="1" alt=""/&gt;</content><summary>Part 3 - Immediate paymentsCloud technology is changing the way payment services are architectured. In this series we will be presenting insight from our customers on adopting open source and cloud technology to modernize their payment service. So far we've presented research-based architectural blueprints of omnichannel customer experience, integrating with SaaS applications, and cloud-native dev...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2020-09-24T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/7r9lufX4aUk/payments-architecture-immediate-payments-example.html</feedburner:origLink></entry><entry><title>Kubernetes: The evolution of distributed systems</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/u8IJBG1sQkc/" /><category term="application runtime" scheme="searchisko:content:tags" /><category term="devnation" scheme="searchisko:content:tags" /><category term="distributed applications" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="Kubernetes dapr" scheme="searchisko:content:tags" /><category term="mecha architecture" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="serverless" scheme="searchisko:content:tags" /><category term="service mesh" scheme="searchisko:content:tags" /><author><name>Bilgin Ibryam</name></author><id>searchisko:content:id:jbossorg_blog-kubernetes_the_evolution_of_distributed_systems</id><updated>2020-09-23T07:00:38Z</updated><published>2020-09-23T07:00:38Z</published><content type="html">&lt;p&gt;DevNation Tech Talks are hosted by the Red Hat technologists who create our products. These sessions include real solutions plus code and sample projects to help you get started. In this talk, you’ll learn about Kubernetes and distributed systems from &lt;a href="https://developers.redhat.com/blog/author/bibryam/"&gt;Bilgin Ibryam&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/author/burrsutter/"&gt;Burr Sutter&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Cloud-native applications of the future will consist of hybrid workloads: stateful applications, batch jobs, stateless microservices, and functions (plus maybe something else) wrapped as Linux &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containers&lt;/a&gt; and deployed via &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; on any cloud. Functions and the so-called &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt; computing model are the latest evolution of what started as service-oriented architecture years ago. But is this the last step of the application architecture evolution and is it here to stay?&lt;/p&gt; &lt;p&gt;&lt;span id="more-787807"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;During this talk, we will take you on a journey exploring distributed application needs, and how they evolved with Kubernetes, &lt;a href="https://developers.redhat.com/topics/service-mesh/"&gt;Istio&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://knative.dev/"&gt;Knative&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://dapr.io/"&gt;Dapr&lt;/a&gt;, and other projects. By the end of the session, you will know what is coming after &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Watch the entire talk:&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/CZPEIJFJV9k?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;Join us at an &lt;a href="https://developers.redhat.com/events/"&gt;upcoming developer event&lt;/a&gt;, and see our collection of &lt;a href="https://developers.redhat.com/devnation/?page=0"&gt;past DevNation Tech Talks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#038;title=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" data-a2a-url="https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/" data-a2a-title="Kubernetes: The evolution of distributed systems"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/"&gt;Kubernetes: The evolution of distributed systems&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/u8IJBG1sQkc" height="1" width="1" alt=""/&gt;</content><summary>DevNation Tech Talks are hosted by the Red Hat technologists who create our products. These sessions include real solutions plus code and sample projects to help you get started. In this talk, you’ll learn about Kubernetes and distributed systems from Bilgin Ibryam and Burr Sutter. Cloud-native applications of the future will consist of hybrid workloads: stateful applications, batch jobs, stateles...</summary><dc:creator>Bilgin Ibryam</dc:creator><dc:date>2020-09-23T07:00:38Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/</feedburner:origLink></entry><entry><title>Troubleshooting user task errors in Red Hat Process Automation Manager and Red Hat JBoss BPM Suite</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qOpKvHTHw4Y/" /><category term="business process automation" scheme="searchisko:content:tags" /><category term="business process notation" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="Drools" scheme="searchisko:content:tags" /><category term="event-driven" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="jBPM" scheme="searchisko:content:tags" /><category term="jbpm troubleshooting" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><author><name>Anton Giertli</name></author><id>searchisko:content:id:jbossorg_blog-troubleshooting_user_task_errors_in_red_hat_process_automation_manager_and_red_hat_jboss_bpm_suite</id><updated>2020-09-22T07:00:19Z</updated><published>2020-09-22T07:00:19Z</published><content type="html">&lt;p&gt;I&amp;#8217;ve been around &lt;a href="https://docs.jboss.org/jbpm/release/7.42.0.Final/jbpm-docs/html_single/#jbpmreleasenotes"&gt;Red Hat JBoss BPM Suite&lt;/a&gt; (jBPM) and &lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Process Automation Manager&lt;/a&gt; (RHPAM) for many years. Over that time, I&amp;#8217;ve learned a lot about the lesser-known aspects of this business process management engine.&lt;/p&gt; &lt;p&gt;If you are like most people, you might believe that user tasks are trivial, and learning about their details is unnecessary. Then, one day, you will find yourself troubleshooting an error like this one:&lt;/p&gt; &lt;pre&gt;User '[User:'admin']' was unable to execution operation 'Start' on task id 287271 due to a no 'current status' match. &lt;/pre&gt; &lt;p&gt;Receiving one too many similar error messages led me to learn everything that I know about user tasks, and I have decided to share my experience.&lt;/p&gt; &lt;p&gt;User tasks are a vital part of any business process management engine, jBPM included. Their behavior is defined by the &lt;a target="_blank" rel="nofollow" href="http://docs.oasis-open.org/bpel4people/ws-humantask-1.1-spec-cs-01.html"&gt;OASIS Web Services—Human Task Specification&lt;/a&gt;, which has been fully adopted by &lt;a target="_blank" rel="nofollow" href="https://www.omg.org/spec/BPMN/2.0/About-BPMN/"&gt;Business Process Model and Notation (BPMN) 2.0&lt;/a&gt;—the standard for business processes diagrams. The spec defines two exceptionally important things that I will discuss in this article: The user task lifecycle and task access control. Without further ado, let&amp;#8217;s jump right in.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: These troubleshooting tips are applicable to Red Hat JBoss BPM Suite 6.2 and above and Red Hat Process Automation Manager 7.&lt;/p&gt; &lt;p&gt;&lt;span id="more-755127"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;The user task lifecycle&lt;/h2&gt; &lt;p&gt;The diagram in Figure 1 illustrates how a task transitions from one state to another, along with the valid executable actions to execute for every state.&lt;/p&gt; &lt;div id="attachment_755137" style="width: 649px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/img_5f11a65136c43.png"&gt;&lt;img aria-describedby="caption-attachment-755137" class="wp-image-755137" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/img_5f11a65136c43.png" alt="Diagram of the user task lifecycle." width="639" height="462" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/img_5f11a65136c43.png 800w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/img_5f11a65136c43-300x217.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/img_5f11a65136c43-768x555.png 768w" sizes="(max-width: 639px) 100vw, 639px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-755137" class="wp-caption-text"&gt;Figure 1: The user task lifecycle. (&lt;em&gt;Source: &lt;a target="_blank" rel="nofollow" href="http://docs.oasis-open.org/bpel4people/ws-humantask-1.1-spec-cs-01.html"&gt;OASIS Web Services – Human Task (WS-HumanTask) Specification Version 1.1&lt;/a&gt;&lt;/em&gt;)&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To see how this diagram can be helpful, consider a practical example. Imagine a &lt;strong&gt;Start &amp;#8211; &amp;#62;User Task &amp;#8211; &amp;#62; end&lt;/strong&gt; process. As shown in Figure 2, the task has only one actor assigned, &lt;code&gt;anton&lt;/code&gt;.&lt;/p&gt; &lt;div id="attachment_784307" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting1.png"&gt;&lt;img aria-describedby="caption-attachment-784307" class="wp-image-784307 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting1-1024x520.png" alt="Implementation/Execution: Task name SampleHT with Actor name &amp;#34;anton&amp;#34;" width="640" height="325" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting1-1024x520.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting1-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting1-768x390.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-784307" class="wp-caption-text"&gt;Figure 2. A simple process for a single user.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Upon starting this process, the task automatically transitions into the Reserved state, which is dictated by the &lt;a target="_blank" rel="nofollow" href="http://docs.oasis-open.org/bpel4people/ws-humantask-1.1-spec-cs-01.html"&gt;WS-Human Task&lt;/a&gt; spec, section 4.10.1: &amp;#8220;When the task has a single potential owner, it transitions into the &lt;em&gt;Reserved&lt;/em&gt; state.&amp;#8221;&lt;/p&gt; &lt;p&gt;Now, let&amp;#8217;s see what happens when we execute the following call:&lt;/p&gt; &lt;pre&gt;$ curl -X PUT -u anton:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/1/states/&lt;strong&gt;completed&lt;/strong&gt;?user=anton" -H "accept: application/json" -H "content-type: application/json" -d "{}" &lt;/pre&gt; &lt;p&gt;We will observe this error:&lt;/p&gt; &lt;pre&gt;Could not commit session: org.jbpm.services.task.exception.PermissionDeniedException: User '[UserImpl:&lt;code&gt;anton&lt;/code&gt;]' was unable to execute operation 'Complete' on task id 1 due to a no 'current status' matchCould not commit session: org.jbpm.services.task.exception.PermissionDeniedException: User '[UserImpl:&lt;code&gt;anton&lt;/code&gt;]' was unable to execute operation 'Complete' on task id 1 due to a no 'current status' match at org.jbpm.services.task.internals.lifecycle.MVELLifeCycleManager.evalCommand(MVELLifeCycleManager.java:163) at org.jbpm.services.task.internals.lifecycle.MVELLifeCycleManager.taskOperation(MVELLifeCycleManager.java:392)&lt;/pre&gt; &lt;p&gt;But there&amp;#8217;s no reason to panic: We can refer to the diagram in Figure 1 to understand what happened. Look closely, and you will see that from the Reserved status, the allowed operation is Start. This moves our task status to InProgress, which lets us execute the Complete operation.&lt;/p&gt; &lt;p&gt;So, to solve this error, we call &lt;code&gt;start&lt;/code&gt; and then &lt;code&gt;complete&lt;/code&gt;. Or, if you can&amp;#8217;t be bothered with this task lifecycle mess, use the &lt;code&gt;auto-progress&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt;$ curl -X PUT -u anton:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/1/states/completed?user=anton&amp;#38;&lt;strong&gt;auto-progress&lt;/strong&gt;=true" -H "accept: application/json" -H "content-type: application/json" -d "{}" &lt;/pre&gt; &lt;p&gt;Internally, the engine falls back to the logic in this &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/7.39.x/jbpm-services/jbpm-kie-services/src/main/java/org/jbpm/kie/services/impl/UserTaskServiceImpl.java#L231"&gt;implementation example&lt;/a&gt; (follow the link for the complete code):&lt;/p&gt; &lt;pre&gt;TaskService taskService = engine.getTaskService(); // auto progress if needed if (task.getStatus().equals(Status.Ready.name())) { taskService.claim(taskId.longValue(), userId); taskService.start(taskId.longValue(), userId); } else if (task.getStatus().equals(Status.Reserved.name())) { taskService.start(taskId.longValue(), userId); } // perform actual operation taskService.complete(taskId, userId, params); &lt;/pre&gt; &lt;p&gt;Based on the current task status, it executes all of the necessary intermediate steps (in our case, that intermediate step was the &lt;strong&gt;Start&lt;/strong&gt; operation). Whether to use &lt;code&gt;autoProgress&lt;/code&gt; depends on your client&amp;#8217;s needs: Some clients require fine-grained distinctions between task states, and some don&amp;#8217;t care. Using &lt;code&gt;autoProgress&lt;/code&gt; certainly simplifies life for you as a developer, but either way, it&amp;#8217;s important to understand what happens behind the scenes.&lt;/p&gt; &lt;h3&gt;Study the source&lt;/h3&gt; &lt;p&gt;You might have noticed that the stack trace above mentions useful classes. If you want to go really deep (which I occasionally do), you can dig into these. Low-level implementation details matter for one simple reason: The source code never lies—and the documentation might. Don&amp;#8217;t trust diagrams, articles, or documentation, for that matter; trust the source code.&lt;/p&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/master/jbpm-human-task/jbpm-human-task-core/src/main/resources/operations-dsl.mvel"&gt;operations-dsl.mvel&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/master/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/internals/lifecycle/MVELLifeCycleManager.java"&gt;MVELLifeCycleManager.java&lt;/a&gt; files hold the &lt;em&gt;actual implementation&lt;/em&gt; of the task lifecycle illustrated in Figure 1.&lt;/p&gt; &lt;p&gt;Hopefully, this covers the lifecycle-related errors and gives you enough information to help with troubleshooting.&lt;/p&gt; &lt;h2&gt;Task access control&lt;/h2&gt; &lt;p&gt;The next important aspect of dealing with user tasks is task access control. Essentially, if you want to execute a task-related action, the user must be eligible to execute that action. For a user to be eligible, the engine must consider that user to be a &lt;em&gt;potential owner&lt;/em&gt; of the task.&lt;/p&gt; &lt;p&gt;The component that plays a vital role in checking task access is &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/droolsjbpm-knowledge/blob/master/kie-api/src/main/java/org/kie/api/task/UserGroupCallback.java"&gt;UserGroupCallback&lt;/a&gt;. It&amp;#8217;s a simple interface, and jBPM allows you to plug in various (even custom) implementations. We&amp;#8217;ll get to &lt;code&gt;UserGroupCallback&lt;/code&gt; later.&lt;/p&gt; &lt;h3&gt;jBPM task access example&lt;/h3&gt; &lt;p&gt;Note that in jBPM, &lt;em&gt;potential owner&lt;/em&gt; refers to both individual actors and groups. To illustrate, imagine a task with one group assigned: &lt;code&gt;sampleGroup&lt;/code&gt;, shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_784337" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting2.png"&gt;&lt;img aria-describedby="caption-attachment-784337" class="wp-image-784337 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting2-1024x580.png" alt="Implementation/Execution: Task Name SampleHT, no actors, Group name sampleGroup" width="640" height="363" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting2-1024x580.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting2-300x170.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/user-task-troubleshooting2-768x435.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-784337" class="wp-caption-text"&gt;Figure 3: Example process with single group as a potential owner&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The task also has two users, which are defined in &lt;a href="https://developers.redhat.com/videos/vimeo/95462201"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt;anton1=kie-server,sampleGroup,admin anton2=kie-server,admin &lt;/pre&gt; &lt;p&gt;Now, we execute a &lt;em&gt;claim&lt;/em&gt; operation that is authenticated with the user &lt;code&gt;anton2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;curl -X PUT -u anton2:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/1/states/claimed?user=anton2" -H "accept: application/json" &lt;/pre&gt; &lt;p&gt;The claim fails, resulting in the following error:&lt;/p&gt; &lt;pre&gt;12:13:20,117 WARN [org.jbpm.services.task.persistence.TaskTransactionInterceptor] (default task-8) Could not commit session: org.jbpm.services.task.exception.PermissionDeniedException: User '[UserImpl:&lt;code&gt;anton2&lt;/code&gt;]' does not have permissions to execute operation 'Claim' on task id 112:13:20,117 WARN [org.jbpm.services.task.persistence.TaskTransactionInterceptor] (default task-8) Could not commit session: org.jbpm.services.task.exception.PermissionDeniedException: User '[UserImpl:&lt;code&gt;anton2&lt;/code&gt;]' does not have permissions to execute operation 'Claim' on task id 1 at org.jbpm.services.task.internals.lifecycle.MVELLifeCycleManager.evalCommand(MVELLifeCycleManager.java:127) at org.jbpm.services.task.internals.lifecycle.MVELLifeCycleManager.taskOperation(MVELLifeCycleManager.java:392) at org.jbpm.services.task.impl.TaskInstanceServiceImpl.claim(TaskInstanceServiceImpl.java:157) at org.jbpm.services.task.commands.ClaimTaskCommand.execute(ClaimTaskCommand.java:52) at org.jbpm.services.task.commands.ClaimTaskCommand.execute(ClaimTaskCommand.java:33)&lt;/pre&gt; &lt;p&gt;Now, this error is expected and obvious because &lt;code&gt;anton2&lt;/code&gt; is not part of &lt;code&gt;sampleGroup&lt;/code&gt;. The jBPM engine does not consider &lt;code&gt;anton2&lt;/code&gt; a potential owner, and so the operation fails. But let&amp;#8217;s &lt;em&gt;really&lt;/em&gt; try to understand what happened during this call. The stack trace gives us all the clues that we need.&lt;/p&gt; &lt;h3&gt;Study the stack trace&lt;/h3&gt; &lt;p&gt;First—before the actual claim— &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/7.39.x/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/commands/ClaimTaskCommand.java#L50"&gt;&lt;code&gt;UserGroupCallback&lt;/code&gt; operation&lt;/a&gt; was executed (follow the link for the complete source code):&lt;/p&gt; &lt;pre&gt;public Void execute(Context cntxt) { TaskContext context = (TaskContext) cntxt; doCallbackUserOperation(userId, context, true); groupIds = doUserGroupCallbackOperation(userId, null, context); context.set("local:groups", groupIds); context.getTaskInstanceService().claim(taskId, userId); return null; } &lt;/pre&gt; &lt;p&gt;We can see it is passing &lt;code&gt;null&lt;/code&gt; as the value for the &lt;code&gt;groups&lt;/code&gt; attribute.&lt;/p&gt; &lt;p&gt;This leads us to the next call, which eventually executes the &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/master/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/commands/UserGroupCallbackTaskCommand.java#L157"&gt;registered &lt;code&gt;UserGroupCallback&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;protected List doCallbackGroupsOperation(String userId, List groupIds, TaskContext context) { ... if (!(userGroupsMap.containsKey(userId) &amp;#38;&amp;#38; userGroupsMap.get(userId).booleanValue())) { &lt;strong&gt;  //usergroupcallback invocation&lt;/strong&gt; &lt;strong&gt; List userGroups = filterGroups(context.getUserGroupCallback().getGroupsForUser(userId));&lt;/strong&gt; if (userGroups != null &amp;#38;&amp;#38; userGroups.size() &amp;#62; 0) { for (String group: userGroups) { addGroupFromCallbackOperation(group, context); } userGroupsMap.put(userId, true); groupIds = userGroups; } } } } else { if (groupIds != null) { for (String groupId: groupIds) { addGroupFromCallbackOperation(groupId, context); } } } return groupIds; } &lt;/pre&gt; &lt;p&gt;We pass &lt;code&gt;userId&lt;/code&gt; to the &lt;code&gt;getGroupsForUser&lt;/code&gt; callback operation, which results in the list of groups to which our user belongs. By default, jBPM is configured with the &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/master/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/identity/JAASUserGroupCallbackImpl.java#L111"&gt;JAASUserGroupCallbackImpl&lt;/a&gt; callback implementation.&lt;/p&gt; &lt;p&gt;This particular callback implementation delegates the &amp;#8220;heavy lifting&amp;#8221; to the underlying web container. The container returns the list of groups for the currently authenticated user. The output for the user &lt;code&gt;anton2&lt;/code&gt; will be a list including &lt;code&gt;kie-server,admin&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If we trace the code execution further to &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/master/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/internals/lifecycle/MVELLifeCycleManager.java#L223"&gt;MVELLifeCycleManager&lt;/a&gt;, the engine will try to find the intersection between potential owners defined on a task (&lt;code&gt;sampleGroup&lt;/code&gt;) and the groups that our authenticated user—&lt;code&gt;anton2&lt;/code&gt;—belongs to:&lt;/p&gt; &lt;pre&gt;private boolean isAllowed(final User user, final List &amp;#60; String &amp;#62; groupIds, final List &amp;#60; OrganizationalEntity &amp;#62; entities) { for (OrganizationalEntity entity: entities) { if (entity instanceof User &amp;#38;&amp;#38; entity.equals(user)) { return true; } if (entity instanceof Group &amp;#38;&amp;#38; groupIds != null &amp;#38;&amp;#38; groupIds.contains(entity.getId())) { return true; } } return false; } &lt;/pre&gt; &lt;p&gt;Our user, &lt;code&gt;anton2&lt;/code&gt;, belongs to the &lt;code&gt;kie-server&lt;/code&gt; and &lt;code&gt;admin&lt;/code&gt; roles. This user does not belong to the &lt;code&gt;sampleGroup&lt;/code&gt;. As a result, the engine determines that &lt;code&gt;anton2&lt;/code&gt; is not a potential owner, and the operation fails with a &lt;code&gt;PermissionDeniedException&lt;/code&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: There are different callback implementations, and your source of truth doesn&amp;#8217;t necessarily need to come from the web container. It can come from an &lt;a target="_blank" rel="nofollow" href="https://ldap.com/"&gt;LDAP&lt;/a&gt; server, a database, a &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat Single Sign-On&lt;/a&gt; (SSO) instance, and so on. But &lt;code&gt;JAASUSerGroupCallback&lt;/code&gt; is a sensible default. If you want to learn more about other out-of-the-box implementations, see the &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/tree/master/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/identity"&gt;following module&lt;/a&gt; with all of its UserGroupCallbacks.&lt;/p&gt; &lt;h2&gt;Simplifying user task access control&lt;/h2&gt; &lt;p&gt;We have discussed the task lifecycle, task access control, and how user-group callbacks fit into the picture. The relationship between these components presents a challenge, especially when using &lt;code&gt;JAASUserGroupCallback&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Imagine these three tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Task1(group1)&lt;/li&gt; &lt;li&gt;Task2(group2)&lt;/li&gt; &lt;li&gt;Task3(group3)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to claim these tasks, you must execute three separate claim operations (well, unless you implement a &lt;code&gt;kie-server&lt;/code&gt; extension to allow multiple claimed tasks at once, but that&amp;#8217;s a separate topic). Moreover, you must authenticate with &lt;em&gt;three different users&lt;/em&gt;, because that is how &lt;code&gt;JAASUserGroupCallback&lt;/code&gt; works. If you are using a Java client (&lt;code&gt;kie-server-client&lt;/code&gt;), you will also have to create three different instances of &lt;code&gt;KieServicesClient&lt;/code&gt;, and you would probably have to cache them to save time.&lt;/p&gt; &lt;p&gt;It&amp;#8217;s likely that you would end up with something like this:&lt;/p&gt; &lt;pre&gt;Map&amp;#60;String,KieServicesClient&amp;#62; clientMap; // String == userId &lt;/pre&gt; &lt;p&gt;Let&amp;#8217;s just say that this code is cumbersome, at best. Fortunately, you can use the following system property to bypass server authentication for a user that is executing a task operation:&lt;/p&gt; &lt;pre&gt;org.kie.server.bypass.auth.user = true &lt;/pre&gt; &lt;p&gt;This &lt;em&gt;bypass property&lt;/em&gt; is a holy grail for our understanding of user task lifecycle and access and will test your knowledge of what you have learned so far.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note for Spring users&lt;/strong&gt;: This is a &lt;em&gt;system property&lt;/em&gt;, not a Spring property, so don&amp;#8217;t put it in &lt;code&gt;application.properties&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Using the bypass property&lt;/h3&gt; &lt;p&gt;So, what can the final implementation look like when using this property? Again, imagine these three tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Task1(group1)&lt;/li&gt; &lt;li&gt;Task2(group2)&lt;/li&gt; &lt;li&gt;Task3(group3)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And imagine the following users defined in JBoss EAP:&lt;/p&gt; &lt;pre&gt;anton1=kie-server,admin,group1 anton2=kie-server,admin,group2 anton3=kie-server,admin,group3 serviceAccount=kie-server,admin &lt;/pre&gt; &lt;p&gt;We still have to call &lt;code&gt;claim&lt;/code&gt; three separate times. But, with the bypass property enabled, we only have to authenticate once (so, we&amp;#8217;ll need just one instance of &lt;code&gt;KieServicesClient&lt;/code&gt;) and we will pass &lt;code&gt;user1&lt;/code&gt;, &lt;code&gt;user2&lt;/code&gt;, and &lt;code&gt;user3&lt;/code&gt; in a query parameter. We are effectively &lt;em&gt;bypassing&lt;/em&gt; the server authentication for these user instances. Internally, the engine either selects the authenticated user or the user from the query parameter, depending on the value of the bypass property. The selected user is then propagated all the way to the &lt;code&gt;UserGroupCallback&lt;/code&gt;. But, again, don&amp;#8217;t just trust me: Check the &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/droolsjbpm-integration/blob/7.39.x/kie-server-parent/kie-server-services/kie-server-services-jbpm/src/main/java/org/kie/server/services/jbpm/UserTaskServiceBase.java#L70"&gt;source code from the &lt;code&gt;kie-server&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;protected String getUser(String queryParamUser) { if (bypassAuthUser &amp;#38;&amp;#38; queryParamUser != null) { return queryParamUser; } return identityProvider.getName(); } &lt;/pre&gt; &lt;p&gt;This is literally it: All the magic behind the bypass property. Changing the input parameter to the &lt;code&gt;UserGroupCallback&lt;/code&gt; method completely changes the task access control behavior.&lt;/p&gt; &lt;p&gt;Now, let&amp;#8217;s test further to see if we understand the bypass correctly.&lt;/p&gt; &lt;p&gt;Consider that the previous three tasks are active. Now we want to claim one of these with a bypass enabled on the server-side:&lt;/p&gt; &lt;pre&gt;$ curl -X PUT -u serviceAccount:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/1/states/claimed?user=anton1" -H "accept: application/json" &lt;/pre&gt; &lt;p&gt;As you can see, we are authenticating with a single user (&lt;code&gt;serviceAccount&lt;/code&gt;) and bypassing the authentication of the user requesting the claim (&lt;code&gt;anton1&lt;/code&gt;). Let&amp;#8217;s pause for a second: What do you think will be the outcome of this operation? We have bypass enabled, so user &lt;code&gt;anton1&lt;/code&gt; will go through as an input to the &lt;code&gt;UserGroupCallback&lt;/code&gt;. It should work. And yet, it ends with this:&lt;/p&gt; &lt;pre&gt;WARN [org.jbpm.services.task.persistence.TaskTransactionInterceptor] (default task-3) Could not commit session: org.jbpm.services.task.exception.PermissionDeniedException: User '[UserImpl:&lt;code&gt;anton1&lt;/code&gt;]' does not have permissions to execute operation 'Claim' on task id 1WARN [org.jbpm.services.task.persistence.TaskTransactionInterceptor] (default task-3) Could not commit session: org.jbpm.services.task.exception.PermissionDeniedException: User '[UserImpl:&lt;code&gt;anton1&lt;/code&gt;]' does not have permissions to execute operation 'Claim' on task id 1 at org.jbpm.services.task.internals.lifecycle.MVELLifeCycleManager.evalCommand(MVELLifeCycleManager.java:127) at org.jbpm.services.task.internals.lifecycle.MVELLifeCycleManager.taskOperation(MVELLifeCycleManager.java:392&lt;/pre&gt; &lt;p&gt;That doesn&amp;#8217;t make sense, does it? In fact, we&amp;#8217;ve just received one of the most confusing errors in the jBPM world. The &lt;code&gt;anton1&lt;/code&gt; user clearly belongs to &lt;code&gt;group1&lt;/code&gt; So, why does the log say the exact opposite? Why has the world stopped making sense?&lt;/p&gt; &lt;p&gt;The bypass behaves like this due to a &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/master/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/identity/JAASUserGroupCallbackImpl.java#L114"&gt;tiny implementation detail of &lt;code&gt;JAASUserGroupCallback&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;public List getGroupsForUser(String userId) { List roles = new ArrayList(); try { Subject subject = getSubjectFromContainer(); ... &lt;/pre&gt; &lt;p&gt;Although we are passing &lt;code&gt;userId&lt;/code&gt; equal to &lt;code&gt;anton1&lt;/code&gt; (which you can see in the error message), the &lt;code&gt;getGroupsForUser&lt;/code&gt; method ignores this parameter. It still gets the actual user from the container. As a result, our authenticated user is &lt;code&gt;serviceAccount&lt;/code&gt;, but &lt;code&gt;serviceAccount&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; a potential owner of this task. There is no intersection between &lt;code&gt;kie-server&lt;/code&gt;,&lt;code&gt;admin&lt;/code&gt;, and &lt;code&gt;group1&lt;/code&gt;, so the call fails.&lt;/p&gt; &lt;p&gt;So, what is the solution? The bypass alone doesn&amp;#8217;t solve anything for us. We have to use it in combination with either a task administrator or a custom &lt;code&gt;UserGroupCallback&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;The task administrator strategy&lt;/h3&gt; &lt;p&gt;In jBPM, the &lt;em&gt;task administrator&lt;/em&gt; is a superuser that is eligible to execute all task operations, even though it isn&amp;#8217;t defined as a potential owner. If we combine our existing &lt;code&gt;serviceAccount&lt;/code&gt; user with the task administrator&amp;#8217;s capabilities, we will achieve the behavior that we desire: The user &lt;code&gt;anton1&lt;/code&gt; will inherit the task admin&amp;#8217;s superpowers from the &lt;code&gt;serviceAccount&lt;/code&gt; user.&lt;/p&gt; &lt;p&gt;By default, the task admin is defined as a user with the name of &lt;code&gt;Administrator&lt;/code&gt;, or a user who is part of the group &lt;code&gt;Administrators&lt;/code&gt;. We can change these two values via the &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/7.39.x/jbpm-human-task/jbpm-human-task-workitems/src/main/java/org/jbpm/services/task/wih/util/PeopleAssignmentHelper.java#L54"&gt;following properties&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;public static final String DEFAULT_ADMIN_USER = System.getProperty("org.jbpm.ht.admin.user", "Administrator"); public static final String DEFAULT_ADMIN_GROUP = System.getProperty("org.jbpm.ht.admin.group", "Administrators"); &lt;/pre&gt; &lt;p&gt;For simplicity, let&amp;#8217;s add the &lt;code&gt;Administrators&lt;/code&gt; role to the &lt;code&gt;serviceAccount&lt;/code&gt; user, and then test again:&lt;/p&gt; &lt;pre&gt;serviceAccount=kie-server,admin,Administrators &lt;/pre&gt; &lt;p&gt;And now this call finally succeeds:&lt;/p&gt; &lt;pre&gt;$ curl -X PUT -u serviceAccount:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/1/states/claimed?user=anton1" -H "accept: application/json" &lt;/pre&gt; &lt;p&gt;We can now use a single user (in this case, &lt;code&gt;serviceAccount&lt;/code&gt;) to authenticate &lt;em&gt;all&lt;/em&gt; of our &lt;code&gt;kie-server&lt;/code&gt; requests. This dramatically simplifies the integration with the &lt;code&gt;kie-server&lt;/code&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are using the &lt;code&gt;kie-server-client&lt;/code&gt; API to interact with the &lt;code&gt;kie-server&lt;/code&gt;, you need to set the &lt;code&gt;org.kie.server.bypass.auth.user&lt;/code&gt; property to &lt;code&gt;true&lt;/code&gt;, even on the client-side. Otherwise, the user you want to bypass will not be passed to the &lt;code&gt;queryParameter&lt;/code&gt;, and you will end up with confusing behavior again.&lt;/p&gt; &lt;h3&gt;Caution&lt;/h3&gt; &lt;p&gt;There is an important consequence of combining the task administrator with the bypass property, which should not come as a surprise at this point.&lt;/p&gt; &lt;p&gt;Imagine that we have &lt;code&gt;Task1(group1)&lt;/code&gt; and &lt;code&gt;user anton2=kie-server,admin,group2&lt;/code&gt;. What do you think will happen after issuing this call?&lt;/p&gt; &lt;pre&gt;$ curl -X PUT -u serviceAccount:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/1/states/claimed?user=anton2" -H "accept: application/json" &lt;/pre&gt; &lt;p&gt;It will succeed, even though &lt;code&gt;anton2&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; a potential owner (this user is not part of &lt;code&gt;group1&lt;/code&gt;). The call succeeds because we have received roles from the authenticated user, which is &lt;code&gt;serviceAccount&lt;/code&gt;. This user has the &lt;code&gt;Administrators&lt;/code&gt; role set, so our user, &lt;code&gt;anton2&lt;/code&gt;, is now a superuser and is thus eligible to execute any action on any task.&lt;/p&gt; &lt;p&gt;Similarly, what do you think will happen after this call?&lt;/p&gt; &lt;pre&gt;$ curl -X PUT -u serviceAccount:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/2/states/claimed?user=totallyRandomNonExistentUser" -H "accept: application/json" &lt;/pre&gt; &lt;p&gt;In this case, we pass a user that does not even exist in the container, and the operation still succeeds. If that is unexpected, it shouldn&amp;#8217;t be. The &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/7.39.x/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/identity/JAASUserGroupCallbackImpl.java"&gt;JAASUserGroupCallbackImpl&lt;/a&gt; only looks for the authenticated user (&lt;code&gt;serviceAccount&lt;/code&gt;). In this case, the user has superpowers, via the &lt;code&gt;Administrators&lt;/code&gt; role. Our &lt;code&gt;totallyRandomNonExistentUser&lt;/code&gt; inherits these powers.&lt;/p&gt; &lt;h3&gt;The custom UserGroupCallback strategy&lt;/h3&gt; &lt;p&gt;If you (or your client) are not willing to accept this behavior, you can choose to use a custom &lt;code&gt;UserGroupCallback&lt;/code&gt;, instead of the task admin. For testing purposes, it will be easiest to reuse the already existing &lt;code&gt;application-roles.properties&lt;/code&gt;. &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/7.39.x/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/identity/JBossUserGroupCallbackImpl.java"&gt;JBossUserGroupCallbackImpl&lt;/a&gt; is already compatible with this file, so at least we don&amp;#8217;t have to implement custom logic to parse it. This callback implementation respects the user passed as an input parameter: It ignores the authenticated user and instead returns a list of roles for this user as defined in a properties file.&lt;/p&gt; &lt;p&gt;Once again, you can use LDAP or a database as the source of your user-group mappings. Here&amp;#8217;s the final configuration:&lt;/p&gt; &lt;pre&gt;&amp;#60;property name="org.kie.server.bypass.auth.user" value="true"/&amp;#62; &amp;#60;property name="jbpm.user.group.mapping" value="file:///Users/agiertli/Documents/work/rhpam77/standalone/configuration/application-roles.properties"/&amp;#62; &amp;#60;property name="org.jbpm.ht.callback" value="props"/&amp;#62; &lt;/pre&gt; &lt;p&gt;The value &lt;code&gt;props&lt;/code&gt; is misleading, but it defaults to &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/7.39.x/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/identity/JBossUserGroupCallbackImpl.java"&gt;JBossUserGroupCallbackImpl&lt;/a&gt;. Again, if you don&amp;#8217;t believe me (which you should not), you can double-check&lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/c4eda77138c976016b301383db1369416b76029b/jbpm-runtime-manager/src/main/java/org/jbpm/runtime/manager/impl/identity/UserDataServiceProvider.java#L80"&gt; the source code&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; } else if ("props".equalsIgnoreCase(USER_CALLBACK_IMPL)) { callback = new JBossUserGroupCallbackImpl(true); &lt;/pre&gt; &lt;p&gt;We can now afford to remove the &lt;code&gt;Administrators&lt;/code&gt; role from the &lt;code&gt;serviceAccount&lt;/code&gt;, so the &lt;code&gt;application-roles.properties&lt;/code&gt; looks like this:&lt;/p&gt; &lt;pre&gt;anton1=kie-server,admin,group1 anton2=kie-server,admin,group2 anton3=kie-server,admin,group3 serviceAccount=kie-server,rest-all &lt;/pre&gt; &lt;p&gt;And the following call works, as well:&lt;/p&gt; &lt;pre&gt;$ curl -X PUT -u serviceAccount:password1! "http://localhost:8080/kie-server/services/rest/server/containers /HumanTaskExamples/tasks/1/states/claimed?user=anton1" -H "accept: application/json" &lt;/pre&gt; &lt;p&gt;I hope it&amp;#8217;s clear why the above configuration works. We pass in &lt;code&gt;anton1&lt;/code&gt; (as opposed to the authenticated user) as the user to the callback. The &lt;a target="_blank" rel="nofollow" href="https://github.com/kiegroup/jbpm/blob/7.39.x/jbpm-human-task/jbpm-human-task-core/src/main/java/org/jbpm/services/task/identity/JBossUserGroupCallbackImpl.java"&gt;JBossUserGroupsCallbackImpl&lt;/a&gt; then parses the &lt;code&gt;application-roles.properties&lt;/code&gt; to give us the list of groups that &lt;code&gt;anton1&lt;/code&gt; belongs to. This list is &lt;code&gt;anton1=kie-server,admin,group1&lt;/code&gt; and the group set on &lt;code&gt;Task1&lt;/code&gt; is &lt;code&gt;group1&lt;/code&gt;. Seeing the intersection between these two components, the engine marks the user &lt;code&gt;anton1&lt;/code&gt; as a potential owner for the &lt;code&gt;claim&lt;/code&gt; operation, so the operation succeeds.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;I hope you now have the sufficient resources to debug, troubleshoot, and configure everything you need in regards to jBPM user tasks. As I&amp;#8217;ve demonstrated with the examples in this article, user tasks can seem trivial, until all of a sudden, they are not. Feel free to share your user task troubleshooting stories in the comments!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#38;linkname=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#38;linkname=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#38;linkname=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#38;linkname=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#38;linkname=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#38;linkname=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#38;linkname=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F22%2Ftroubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite%2F&amp;#038;title=Troubleshooting%20user%20task%20errors%20in%20Red%20Hat%20Process%20Automation%20Manager%20and%20Red%20Hat%20JBoss%20BPM%20Suite" data-a2a-url="https://developers.redhat.com/blog/2020/09/22/troubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite/" data-a2a-title="Troubleshooting user task errors in Red Hat Process Automation Manager and Red Hat JBoss BPM Suite"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/22/troubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite/"&gt;Troubleshooting user task errors in Red Hat Process Automation Manager and Red Hat JBoss BPM Suite&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qOpKvHTHw4Y" height="1" width="1" alt=""/&gt;</content><summary>I’ve been around Red Hat JBoss BPM Suite (jBPM) and Red Hat Process Automation Manager (RHPAM) for many years. Over that time, I’ve learned a lot about the lesser-known aspects of this business process management engine. If you are like most people, you might believe that user tasks are trivial, and learning about their details is unnecessary. Then, one day, you will find yourself troubleshooting ...</summary><dc:creator>Anton Giertli</dc:creator><dc:date>2020-09-22T07:00:19Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/22/troubleshooting-user-task-errors-in-red-hat-process-automation-manager-and-red-hat-jboss-bpm-suite/</feedburner:origLink></entry><entry><title>Migrating from Fabric8 Maven Plugin to Eclipse JKube 1.0.0</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jIRCzO9_qRY/" /><category term="apache maven" scheme="searchisko:content:tags" /><category term="cloud native java" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="devops" scheme="searchisko:content:tags" /><category term="fabric8" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Jkube" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="kubernetes maven" scheme="searchisko:content:tags" /><category term="OpenJDK 11" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="openshift maven" scheme="searchisko:content:tags" /><author><name>Rohan Kumar</name></author><id>searchisko:content:id:jbossorg_blog-migrating_from_fabric8_maven_plugin_to_eclipse_jkube_1_0_0</id><updated>2020-09-21T07:00:19Z</updated><published>2020-09-21T07:00:19Z</published><content type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/blog/2020/09/09/cloud-native-java-applications-made-easy-eclipse-jkube-1-0-0-now-available/"&gt;recent release of Eclipse JKube 1.0.0&lt;/a&gt; means that the &lt;a target="_blank" rel="nofollow" href="https://github.com/fabric8io/fabric8-maven-plugin"&gt;Fabric8 Maven Plugin&lt;/a&gt; is no longer supported. If you are currently using the Fabric8 Maven Plugin, this article provides instructions for migrating to &lt;a target="_blank" rel="nofollow" href="https://www.eclipse.org/jkube/"&gt;JKube&lt;/a&gt; instead. I will also explain the relationship between Eclipse JKube and the Fabric8 Maven Plugin (they&amp;#8217;re the same thing) and introduce the highlights of the new Eclipse JKube 1.0.0 release. These migration instructions are for developers working on the &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; platforms.&lt;/p&gt; &lt;h2&gt;Eclipse JKube is the Fabric8 Maven Plugin&lt;/h2&gt; &lt;p&gt;Eclipse JKube and the Fabric8 Maven Plugin are one and the same. Eclipse JKube was first released in 2014 under the name of Fabric8 Maven Plugin. The development team changed the name when we pre-released Eclipse JKube 0.1.0 in December 2019. For more about the name change, see my recent &lt;a href="https://developers.redhat.com/blog/2020/01/28/introduction-to-eclipse-jkube-java-tooling-for-kubernetes-and-red-hat-openshift/"&gt;introduction to Eclipse JKube&lt;/a&gt;. This article focuses on the migration path to JKube 1.0.0.&lt;/p&gt; &lt;p&gt;&lt;span id="more-781287"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;What&amp;#8217;s new in Eclipse JKube 1.0.0&lt;/h2&gt; &lt;p&gt;If you are hesitant about migrating to JKube, the following highlights from the new 1.0.0 release might change your mind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Targeted migration with the new &lt;a target="_blank" rel="nofollow" href="https://search.maven.org/artifact/org.eclipse.jkube/kubernetes-maven-plugin"&gt;Kubernetes Maven Plugin&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://search.maven.org/search?q=a:openshift-maven-plugin"&gt;OpenShift Maven Plugin&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://quay.io/organization/jkube"&gt;Improved base images&lt;/a&gt; based on &lt;a href="https://developers.redhat.com/articles/ubi-faq"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBI) and &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;JDK 11&lt;/a&gt;. (Fabric8 Maven Plugin&amp;#8217;s images are based on JDK 8.)&lt;/li&gt; &lt;li&gt;No coupling to the &lt;a target="_blank" rel="nofollow" href="http://fabric8.io/"&gt;fabric8.io&lt;/a&gt; platform.&lt;/li&gt; &lt;li&gt;Source-to-Image (S2I) support for web applications and &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Better support for &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerizing&lt;/a&gt; your applications with &lt;a target="_blank" rel="nofollow" href="https://www.eclipse.org/jkube/docs/kubernetes-maven-plugin#_jib_java_image_builder"&gt;JIB integration&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://www.eclipse.org/jkube/docs/kubernetes-maven-plugin#jkube:helm"&gt;Helm support&lt;/a&gt; in the Kubernetes Maven Plugin and OpenShift Maven Plugin.&lt;/li&gt; &lt;li&gt;Based on &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/jkube/tree/master/jkube-kit"&gt;JKube Kit&lt;/a&gt;, which is independent of both &lt;a target="_blank" rel="nofollow" href="https://maven.apache.org/"&gt;Maven&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://gradle.org/"&gt;Gradle&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Support for &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/jkube/issues/66"&gt;Podman and Buildah&lt;/a&gt; is coming soon in Eclipse JKube 1.1.0.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fabric8 Maven Plugin generates both Kubernetes and Red Hat OpenShift artifacts, and it automatically detects and deploys resources to the underlying cluster. But developers who use Kubernetes don&amp;#8217;t need OpenShift artifacts, and OpenShift developers don&amp;#8217;t need Kubernetes manifests. We addressed this issue by splitting Fabric8 Maven Plugin into two plugins for &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/jkube"&gt;Eclipse JKube&lt;/a&gt;: Kubernetes Maven Plugin and OpenShift Maven Plugin.&lt;/p&gt; &lt;h2&gt;Eclipse JKube migration made easy&lt;/h2&gt; &lt;p&gt;Eclipse JKube has a &lt;code&gt;migrate&lt;/code&gt; goal that automatically updates Fabric8 Maven Plugin references in your &lt;code&gt;pom.xml&lt;/code&gt; to the Kubernetes Maven Plugin or OpenShift Maven Plugin. In the next sections, I&amp;#8217;ll show you how to migrate a Fabric8 Maven Plugin-based project to either platform.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Untitled-Diagram.jpg"&gt;&lt;img class=" aligncenter wp-image-781297 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Untitled-Diagram-1024x560.jpg" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Untitled-Diagram-1024x560.jpg" alt="Replace the code for the Fabric8 Maven plugin with either the code for the Kubernetes Maven plugin or the OpenShft Maven plugin." width="640" height="350" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Untitled-Diagram-1024x560.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Untitled-Diagram-300x164.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Untitled-Diagram-768x420.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Untitled-Diagram.jpg 1126w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For demonstration purposes, we can use my old random generator application, which displays a random JSON response at a &lt;code&gt;/random&lt;/code&gt; endpoint. To start, clone this repository:&lt;/p&gt; &lt;pre&gt;$ git clone https://github.com/rohanKanojia/fmp-demo-project.git cd fmp-demo-project &lt;/pre&gt; &lt;p&gt;Then build the project:&lt;/p&gt; &lt;pre&gt;$ mvn clean install &lt;/pre&gt; &lt;h2&gt;Eclipse JKube migration for Kubernetes users&lt;/h2&gt; &lt;p&gt;Use the following goal to migrate to Eclipse JKube&amp;#8217;s Kubernetes Maven Plugin. Note that we have to specify a complete &lt;code&gt;artifactId&lt;/code&gt; and &lt;code&gt;groupId&lt;/code&gt; because the plugin is not automatically included in the &lt;code&gt;pom.xml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ mvn org.eclipse.jkube:kubernetes-maven-plugin:migrate &lt;/pre&gt; &lt;p&gt;Here are the logs for the &lt;code&gt;migrate&lt;/code&gt; goal:&lt;/p&gt; &lt;pre&gt;fmp-demo-project : $ mvn org.eclipse.jkube:kubernetes-maven-plugin:migrate [INFO] Scanning for projects... [INFO] [INFO] ----------------------&amp;#60; meetup:random-generator &amp;#62;----------------------- [INFO] Building random-generator 0.0.1 [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- kubernetes-maven-plugin:1.0.0-rc-1:migrate (default-cli) @ random-generator --- [INFO] k8s: Found Fabric8 Maven Plugin in pom with version 4.4.1 [INFO] k8s: Renamed src/main/fabric8 to src/main/jkube [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3.154 s [INFO] Finished at: 2020-09-08T19:32:01+05:30 [INFO] ------------------------------------------------------------------------ fmp-demo-project : $ &lt;/pre&gt; &lt;p&gt;You&amp;#8217;ll notice that all of the Fabric8 Maven Plugin references have been replaced by references to Eclipse JKube. The Kubernetes Maven Plugin is the same as the Fabric8 Maven Plugin. The only differences are the &lt;code&gt;k8s&lt;/code&gt; prefix and that it generates Kubernetes manifests.&lt;/p&gt; &lt;p&gt;Once you&amp;#8217;ve installed the Kubernetes Maven Plugin, you can deploy your application as usual:&lt;/p&gt; &lt;pre&gt;$ mvn k8s:build k8s:resource k8s:deploy &lt;/pre&gt; &lt;h2&gt;Eclipse JKube migration for OpenShift users&lt;/h2&gt; &lt;p&gt;Use the same migration process for the OpenShift Maven Plugin as you would for the Kubernetes Maven Plugin. Run the &lt;code&gt;migrate&lt;/code&gt; goal but with the OpenShift MavenPlugin specified:&lt;/p&gt; &lt;pre&gt;$ mvn org.eclipse.jkube:openshift-maven-plugin:migrate &lt;/pre&gt; &lt;p&gt;Here are the logs for this &lt;code&gt;migrate&lt;/code&gt; goal:&lt;/p&gt; &lt;pre&gt;fmp-demo-project : $ mvn org.eclipse.jkube:openshift-maven-plugin:migrate [INFO] Scanning for projects... [INFO] [INFO] ----------------------&amp;#60; meetup:random-generator &amp;#62;----------------------- [INFO] Building random-generator 0.0.1 [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- openshift-maven-plugin:1.0.0-rc-1:migrate (default-cli) @ random-generator --- [INFO] k8s: Found Fabric8 Maven Plugin in pom with version 4.4.1 [INFO] k8s: Renamed src/main/fabric8 to src/main/jkube [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 4.227 s [INFO] Finished at: 2020-09-08T19:41:34+05:30 [INFO] ------------------------------------------------------------------------ &lt;/pre&gt; &lt;p&gt;This goal replaces all of your Fabric8 Maven Plugin references with references to &lt;a target="_blank" rel="nofollow" href="https://www.eclipse.org/jkube/docs/openshift-maven-plugin"&gt;OpenShift Maven Plugin&lt;/a&gt;. You can then deploy your application to Red Hat OpenShift just as you normally would:&lt;/p&gt; &lt;pre&gt;$ mvn oc:build oc:resource oc:deploy &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;See the &lt;a target="_blank" rel="nofollow" href="https://www.eclipse.org/jkube/docs/migration-guide/"&gt;Eclipse JKube migration guide&lt;/a&gt; for more about migrating from the Fabric8 Maven Plugin on OpenShift or Kubernetes. Feel free to create a &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/jkube/issues/new"&gt;GitHub issue&lt;/a&gt; to report any problems that you encounter during the migration. We really value your feedback, so please report bugs, ask for improvements, and tell us about your migration experience.&lt;/p&gt; &lt;p&gt;Whether you are already using Eclipse JKube or just curious about it, don&amp;#8217;t be shy about joining our welcoming community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Provide feedback on &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/jkube/issues"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Craft code and &lt;a target="_blank" rel="nofollow" href="https://github.com/quarkusio/quarkus/pulls"&gt;push&lt;/a&gt; a &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/jkube/pulls"&gt;pull request&lt;/a&gt; to the Eclipse JKube repository.&lt;/li&gt; &lt;li&gt;Interact with the Eclipse JKube team on &lt;a target="_blank" rel="nofollow" href="https://gitter.im/eclipse/jkube"&gt;Gitter&lt;/a&gt; and the &lt;a target="_blank" rel="nofollow" href="https://accounts.eclipse.org/mailing-list/jkube-dev"&gt;JKube mailing list&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Ask questions and get answers on &lt;a target="_blank" rel="nofollow" href="https://stackoverflow.com/questions/tagged/jkube"&gt;Stack Overflow&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#38;linkname=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#38;linkname=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#38;linkname=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#38;linkname=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#38;linkname=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#38;linkname=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#38;linkname=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F21%2Fmigrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0%2F&amp;#038;title=Migrating%20from%20Fabric8%20Maven%20Plugin%20to%20Eclipse%20JKube%201.0.0" data-a2a-url="https://developers.redhat.com/blog/2020/09/21/migrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0/" data-a2a-title="Migrating from Fabric8 Maven Plugin to Eclipse JKube 1.0.0"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/21/migrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0/"&gt;Migrating from Fabric8 Maven Plugin to Eclipse JKube 1.0.0&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jIRCzO9_qRY" height="1" width="1" alt=""/&gt;</content><summary>The recent release of Eclipse JKube 1.0.0 means that the Fabric8 Maven Plugin is no longer supported. If you are currently using the Fabric8 Maven Plugin, this article provides instructions for migrating to JKube instead. I will also explain the relationship between Eclipse JKube and the Fabric8 Maven Plugin (they’re the same thing) and introduce the highlights of the new Eclipse JKube 1.0.0 relea...</summary><dc:creator>Rohan Kumar</dc:creator><dc:date>2020-09-21T07:00:19Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/21/migrating-from-fabric8-maven-plugin-to-eclipse-jkube-1-0-0/</feedburner:origLink></entry></feed>
